{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwI4hsfMS4Ms",
        "outputId": "01e7025f-c132-4bfe-d901-111c23d0968f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HtNvKUDflWND",
        "outputId": "03a56823-c90d-4765-c1f0-3b0110e4f665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /home/youssef/vllm_env/lib/python3.11/site-packages (from rank_bm25) (2.1.2)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pUZmAi6jlaYV",
        "outputId": "64d6776b-a086-4d5f-ce36-da85477ee0d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama_index\n",
            "  Downloading llama_index-0.12.35-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama_index.embeddings.huggingface\n",
            "  Downloading llama_index_embeddings_huggingface-0.5.4-py3-none-any.whl.metadata (458 bytes)\n",
            "Collecting llama-index-agent-openai<0.5,>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_agent_openai-0.4.7-py3-none-any.whl.metadata (438 bytes)\n",
            "Collecting llama-index-cli<0.5,>=0.4.1 (from llama_index)\n",
            "  Downloading llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13,>=0.12.35 (from llama_index)\n",
            "  Downloading llama_index_core-0.12.35-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4,>=0.3.0 (from llama_index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-llms-openai<0.4,>=0.3.0 (from llama_index)\n",
            "  Downloading llama_index_llms_openai-0.3.38-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5,>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4,>=0.3.0 (from llama_index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4,>=0.3.0 (from llama_index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5,>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_readers_file-0.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting nltk>3.8.1 (from llama_index)\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: openai>=1.14.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama_index) (1.78.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.35->llama_index) (3.11.18)\n",
            "Collecting aiosqlite (from llama-index-core<0.13,>=0.12.35->llama_index)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting banks<3,>=2.0.0 (from llama-index-core<0.13,>=0.12.35->llama_index)\n",
            "  Downloading banks-2.1.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13,>=0.12.35->llama_index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.35->llama_index) (1.2.18)\n",
            "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.13,>=0.12.35->llama_index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.13,>=0.12.35->llama_index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.35->llama_index) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.35->llama_index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.35->llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.35->llama_index) (3.3)\n",
            "Requirement already satisfied: numpy in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.35->llama_index) (2.1.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.35->llama_index) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.35->llama_index) (2.11.4)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.35->llama_index) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.35->llama_index) (2.32.3)\n",
            "Collecting sqlalchemy>=1.4.49 (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.35->llama_index)\n",
            "  Downloading sqlalchemy-2.0.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.2.0 (from llama-index-core<0.13,>=0.12.35->llama_index)\n",
            "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.35->llama_index) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.35->llama_index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.35->llama_index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13,>=0.12.35->llama_index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.35->llama_index) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.35->llama_index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/youssef/vllm_env/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.35->llama_index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.35->llama_index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/youssef/vllm_env/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.35->llama_index) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/youssef/vllm_env/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.35->llama_index) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.35->llama_index) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.35->llama_index) (1.20.0)\n",
            "Collecting griffe (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.35->llama_index)\n",
            "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: jinja2 in /home/youssef/vllm_env/lib/python3.11/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.35->llama_index) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /home/youssef/vllm_env/lib/python3.11/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.35->llama_index) (4.3.8)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama_index) (4.13.4)\n",
            "Collecting pandas (from llama-index-readers-file<0.5,>=0.4.0->llama_index)\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5,>=0.4.0->llama_index)\n",
            "  Downloading pypdf-5.5.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5,>=0.4.0->llama_index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/youssef/vllm_env/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /home/youssef/vllm_env/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /home/youssef/vllm_env/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (3.10)\n",
            "Requirement already satisfied: certifi in /home/youssef/vllm_env/lib/python3.11/site-packages (from httpx->llama-index-core<0.13,>=0.12.35->llama_index) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /home/youssef/vllm_env/lib/python3.11/site-packages (from httpx->llama-index-core<0.13,>=0.12.35->llama_index) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /home/youssef/vllm_env/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.35->llama_index) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.35->llama_index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /home/youssef/vllm_env/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.35->llama_index) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.35->llama_index) (0.4.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (0.31.1)\n",
            "Collecting sentence-transformers>=2.6.1 (from llama_index.embeddings.huggingface)\n",
            "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /home/youssef/vllm_env/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (3.18.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/youssef/vllm_env/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (25.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index.embeddings.huggingface) (1.1.1)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud-0.1.21-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_parse-0.6.22-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting llama-cloud-services>=0.6.22 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud_services-0.6.22-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-cloud-services>=0.6.22->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index) (8.2.0)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud-0.1.19-py3-none-any.whl.metadata (902 bytes)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /home/youssef/vllm_env/lib/python3.11/site-packages (from llama-cloud-services>=0.6.22->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index) (1.1.0)\n",
            "Collecting joblib (from nltk>3.8.1->llama_index)\n",
            "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/youssef/vllm_env/lib/python3.11/site-packages (from nltk>3.8.1->llama_index) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/youssef/vllm_env/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.35->llama_index) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/youssef/vllm_env/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.35->llama_index) (2.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (4.51.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (2.6.0)\n",
            "Collecting scikit-learn (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface)\n",
            "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: scipy in /home/youssef/vllm_env/lib/python3.11/site-packages (from sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.15.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/youssef/vllm_env/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/youssef/vllm_env/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (0.5.3)\n",
            "Collecting greenlet>=1 (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.35->llama_index)\n",
            "  Downloading greenlet-3.2.2-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/youssef/vllm_env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/youssef/vllm_env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/youssef/vllm_env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/youssef/vllm_env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/youssef/vllm_env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/youssef/vllm_env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/youssef/vllm_env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/youssef/vllm_env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/youssef/vllm_env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/youssef/vllm_env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/youssef/vllm_env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/youssef/vllm_env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/youssef/vllm_env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/youssef/vllm_env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.35->llama_index)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13,>=0.12.35->llama_index)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.35->llama_index)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/youssef/vllm_env/lib/python3.11/site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.35->llama_index) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/youssef/vllm_env/lib/python3.11/site-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama_index)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama_index)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /home/youssef/vllm_env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5,>=0.4.0->llama_index) (1.17.0)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers>=2.6.1->llama_index.embeddings.huggingface)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading llama_index-0.12.35-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_agent_openai-0.4.7-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.12.35-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading banks-2.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_llms_openai-0.3.38-py3-none-any.whl (23 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.7-py3-none-any.whl (40 kB)\n",
            "Downloading pypdf-5.5.0-py3-none-any.whl (303 kB)\n",
            "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_embeddings_huggingface-0.5.4-py3-none-any.whl (8.9 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading llama_parse-0.6.22-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_cloud_services-0.6.22-py3-none-any.whl (37 kB)\n",
            "Downloading llama_cloud-0.1.19-py3-none-any.whl (263 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
            "Downloading sqlalchemy-2.0.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m651.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.2.2-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (583 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m583.9/583.9 kB\u001b[0m \u001b[31m829.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m622.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m659.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: striprtf, pytz, filetype, dirtyjson, tzdata, threadpoolctl, tenacity, pypdf, mypy-extensions, marshmallow, joblib, greenlet, colorama, aiosqlite, typing-inspect, sqlalchemy, scikit-learn, pandas, nltk, griffe, llama-cloud, dataclasses-json, banks, llama-index-core, sentence-transformers, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama_index.embeddings.huggingface, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39/39\u001b[0m [llama_index]\u001b[0m [llama_index]services]ile]\n",
            "\u001b[1A\u001b[2KSuccessfully installed aiosqlite-0.21.0 banks-2.1.2 colorama-0.4.6 dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 greenlet-3.2.2 griffe-1.7.3 joblib-1.5.0 llama-cloud-0.1.19 llama-cloud-services-0.6.22 llama-index-agent-openai-0.4.7 llama-index-cli-0.4.1 llama-index-core-0.12.35 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.11 llama-index-llms-openai-0.3.38 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.7 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.22 llama_index-0.12.35 llama_index.embeddings.huggingface-0.5.4 marshmallow-3.26.1 mypy-extensions-1.1.0 nltk-3.9.1 pandas-2.2.3 pypdf-5.5.0 pytz-2025.2 scikit-learn-1.6.1 sentence-transformers-4.1.0 sqlalchemy-2.0.40 striprtf-0.0.26 tenacity-9.1.2 threadpoolctl-3.6.0 typing-inspect-0.9.0 tzdata-2025.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install llama_index llama_index.embeddings.huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxbDH8ykkl_Y",
        "outputId": "ca91b2ad-09b3-4248-d69d-1c2539d24bc9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/youssef/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/youssef/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building Hybrid Retriever from Legal Knowledge Base\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/youssef/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "# For sparse retrieval\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from rank_bm25 import BM25Plus\n",
        "\n",
        "# For dense retrieval\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document\n",
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.vector_stores import SimpleVectorStore\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print(\"Building Hybrid Retriever from Legal Knowledge Base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load the knowledge base previously prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vC4_SLkksla",
        "outputId": "731b2437-ad03-47a8-f1a3-94f05ee9c3ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading legal knowledge base...\n",
            "Found 6 code folders: constitution_marocaine_2011, code_penale_2018, code_obligation_contrats_2019, code_travail_2011, code_comerce_2019, code_famille_2016\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing code folders:   0%|          | 0/6 [00:00<?, ?it/s]\n",
            "Processing constitution_marocaine_2011 files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "                                                                                   \u001b[A\n",
            "Processing code_penale_2018 files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "                                                                        \u001b[A\n",
            "Processing code_obligation_contrats_2019 files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "                                                                                     \u001b[A\n",
            "Processing code_travail_2011 files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "                                                                         \u001b[A\n",
            "Processing code_comerce_2019 files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "                                                                         \u001b[A\n",
            "Processing code_famille_2016 files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Processing code folders: 100%|██████████| 6/6 [00:00<00:00, 81.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1285 legal documents from 6 legal codes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def load_knowledge_base(base_dir='/content/drive/MyDrive/legal-rag-assistant/knowledge_base/Codes'):\n",
        "    \"\"\"Load the legal knowledge base DataFrame\"\"\"\n",
        "    data = []\n",
        "\n",
        "    # Track statistics for reporting\n",
        "    stats = {\n",
        "        'total_files': 0,\n",
        "        'processed_files': 0,\n",
        "        'error_files': 0,\n",
        "        'total_documents': 0,\n",
        "        'codes': set()\n",
        "    }\n",
        "\n",
        "    # Check if the directory exists\n",
        "    if not os.path.exists(base_dir):\n",
        "        print(f\"Directory does not exist: {base_dir}\")\n",
        "        return pd.DataFrame(), stats\n",
        "\n",
        "    # List all code folders\n",
        "    all_folders = [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))]\n",
        "    print(f\"Found {len(all_folders)} code folders: {', '.join(all_folders)}\")\n",
        "\n",
        "    for code_folder in tqdm(all_folders, desc=\"Processing code folders\"):\n",
        "        folder_path = os.path.join(base_dir, code_folder)\n",
        "        if not os.path.isdir(folder_path) or code_folder == 'combined':\n",
        "            continue\n",
        "\n",
        "        stats['codes'].add(code_folder)\n",
        "\n",
        "        all_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
        "        stats['total_files'] += len(all_files)\n",
        "\n",
        "        for file in tqdm(all_files, desc=f\"Processing {code_folder} files\", leave=False):\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                try:\n",
        "                    docs = json.load(f)\n",
        "                    # If it's a list of articles\n",
        "                    if isinstance(docs, list):\n",
        "                        for doc in docs:\n",
        "                            doc['code'] = code_folder\n",
        "                            doc['source_file'] = file\n",
        "                            data.append(doc)\n",
        "                            stats['total_documents'] += 1\n",
        "                    # If it's a dict of articles\n",
        "                    elif isinstance(docs, dict):\n",
        "                        for k, doc in docs.items():\n",
        "                            doc['code'] = code_folder\n",
        "                            doc['source_file'] = file\n",
        "                            data.append(doc)\n",
        "                            stats['total_documents'] += 1\n",
        "                    stats['processed_files'] += 1\n",
        "                except Exception as e:\n",
        "                    stats['error_files'] += 1\n",
        "                    print(f\"Error loading {file_path}: {e}\")\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Check for the combined comprehensive knowledge base\n",
        "    comprehensive_path = os.path.join(base_dir, \"comprehensive_knowledge_base.parquet\")\n",
        "    if os.path.exists(comprehensive_path):\n",
        "        print(f\"Found comprehensive knowledge base at {comprehensive_path}\")\n",
        "        df_comprehensive = pd.read_parquet(comprehensive_path)\n",
        "        print(f\"Loading comprehensive knowledge base with {len(df_comprehensive)} documents\")\n",
        "        return df_comprehensive, stats\n",
        "\n",
        "    return df, stats\n",
        "\n",
        "# Load the knowledge base\n",
        "print(\"Loading legal knowledge base...\")\n",
        "df, stats = load_knowledge_base()\n",
        "\n",
        "print(f\"\\nLoaded {len(df)} legal documents from {len(stats['codes'])} legal codes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "oqsLd8oZmGtM",
        "outputId": "387b6f3a-78f5-4308-bbe3-6d94695f6955"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1285,\n  \"fields\": [\n    {\n      \"column\": \"code\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"constitution_marocaine_2011\",\n          \"code_penale_2018\",\n          \"code_famille_2016\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"livre\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 31,\n        \"samples\": [\n          \"LIVRE IV: DE LA CAPACITE ET DE LA REPRESENTATION LEGALE\",\n          \"LIVRE PREMIER DES PEINES ET DES MESURES DE SURETE\",\n          \"LIVRE II: LE FONDS DE COMMERCE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"titre\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 61,\n        \"samples\": [\n          \"De l'organisation du Parlement\",\n          \"De l'ind\\u00e9pendance de la justice\",\n          \"TITRE II: DE LA GARDE DE L'ENFANT (HADANA)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chapitre\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 93,\n        \"samples\": [\n          \"Chapitre Il : Du r\\u00e8glement int\\u00e9rieur\",\n          \"CHAPITRE PREMIER : DES OBLIGATIONS QUI D\\u00c9RIVENT \\nDES CONVENTIONS ET AUTRES D\\u00c9CLARATIONS DE \\nVOLONT\\u00c9\",\n          \"CHAPITRE PREMIER: DES FIAN\\u00c7AILLES\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"section\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 82,\n        \"samples\": [\n          \"Section II : De l'offre\",\n          \"SECTION I DES PERSONNES RESPONSABLES\",\n          \"SECTION I OUTRAGES ET VIOLENCES A FONCTIONNAIRE PUBLIC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"article_no\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 442,\n        \"samples\": [\n          \"Article 262\",\n          \"Article 207\",\n          \"Article 72\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1275,\n        \"samples\": [\n          \"L'usufruit appartient \\u00e0 celui des l\\u00e9gataires existant au moment du\\nd\\u00e9c\\u00e8s du testateur ou post\\u00e9rieurement \\u00e0 celui-ci. Tout l\\u00e9gataire qui. se\\nr\\u00e9v\\u00e8le apr\\u00e8s le d\\u00e9c\\u00e8s concourt au b\\u00e9n\\u00e9fice de l'usufruit, jusqu'au jour o\\u00f9\\ndevient certaine l'inexistence d'autres l\\u00e9gataires. Les l\\u00e9gataires existants\\nrecueillent alors la nue-propri\\u00e9t\\u00e9 et l'usufruit ; la part de celui d'entre eux\\nqui viendra \\u00e0 d\\u00e9c\\u00e9der, fera partie de sa propre succession.\",\n          \"L'attentat contre la vie ou la personne du Roi est puni de mort.\\nCet attentat n'est jamais excusable.\",\n          \"L'employeur doit d\\u00e9livrer au salari\\u00e9 un certificat de travail, \\u00e0 la \\ncessation du contrat de travail, dans un d\\u00e9lai maximum de huit jours, \\nsous peine de dommages-int\\u00e9r\\u00eats.\\nLe certificat de travail doit exclusivement indiquer la date de l'entr\\u00e9e \\ndu salari\\u00e9 dans l'entreprise, celle de sa sortie et les postes de travail qu'il \\na occup\\u00e9s. Toutefois, par accord entre les deux parties, le certificat de \\ntravail peut comporter des mentions relatives aux qualifications \\nprofessionnelles du salari\\u00e9 et aux services qu'il a rendus.\\nLe certificat de travail est exempt\\u00e9 des droits d'enregistrement \\nm\\u00eame s'il comporte des indications autres que celles pr\\u00e9vues au \\ndeuxi\\u00e8me alin\\u00e9a ci-dessus. L'exemption s'\\u00e9tend au certificat portant la \\nmention de : \\\" libre de tout engagement \\\" ou toute autre formule \\n\\u00e9tablissant que le contrat de travail a pris fin de mani\\u00e8re ordinaire.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source_file\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Constitution_marocaine_2011_full.json\",\n          \"code_penale_2018.json\",\n          \"code_famille_2016_full.json\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a20cda2d-58f0-4f3d-9a83-d1ac2139c728\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code</th>\n",
              "      <th>livre</th>\n",
              "      <th>titre</th>\n",
              "      <th>chapitre</th>\n",
              "      <th>section</th>\n",
              "      <th>article_no</th>\n",
              "      <th>text</th>\n",
              "      <th>source_file</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>constitution_marocaine_2011</td>\n",
              "      <td>Préambule</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Préambule</td>\n",
              "      <td>Fidèle à son choix irréversible de construire ...</td>\n",
              "      <td>Constitution_marocaine_2011_full.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>constitution_marocaine_2011</td>\n",
              "      <td>TITRE PREMIER DISPOSITIONS GENERALES</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Article premier</td>\n",
              "      <td>Le Maroc est une monarchie constitutionnelle, ...</td>\n",
              "      <td>Constitution_marocaine_2011_full.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>constitution_marocaine_2011</td>\n",
              "      <td>TITRE PREMIER DISPOSITIONS GENERALES</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Article 2</td>\n",
              "      <td>La souveraineté appartient à la Nation qui l'e...</td>\n",
              "      <td>Constitution_marocaine_2011_full.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>constitution_marocaine_2011</td>\n",
              "      <td>TITRE PREMIER DISPOSITIONS GENERALES</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Article 3</td>\n",
              "      <td>L'Islam est la religion de l'Etat, qui garanti...</td>\n",
              "      <td>Constitution_marocaine_2011_full.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>constitution_marocaine_2011</td>\n",
              "      <td>TITRE PREMIER DISPOSITIONS GENERALES</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Article 4</td>\n",
              "      <td>L'emblème du Royaume est le drapeau rouge frap...</td>\n",
              "      <td>Constitution_marocaine_2011_full.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1280</th>\n",
              "      <td>code_famille_2016</td>\n",
              "      <td>LIVRE VII: DISPOSITIONS TRANSITOIRES ET FINALES</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Article 396</td>\n",
              "      <td>Les délais prévus par le présent Code sont des...</td>\n",
              "      <td>code_famille_2016_full.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1281</th>\n",
              "      <td>code_famille_2016</td>\n",
              "      <td>LIVRE VII: DISPOSITIONS TRANSITOIRES ET FINALES</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Article 397</td>\n",
              "      <td>Sont abrogées toutes les dispositions contrair...</td>\n",
              "      <td>code_famille_2016_full.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1282</th>\n",
              "      <td>code_famille_2016</td>\n",
              "      <td>LIVRE VII: DISPOSITIONS TRANSITOIRES ET FINALES</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Article 398</td>\n",
              "      <td>Demeurent valables, les actes de procédures ef...</td>\n",
              "      <td>code_famille_2016_full.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1283</th>\n",
              "      <td>code_famille_2016</td>\n",
              "      <td>LIVRE VII: DISPOSITIONS TRANSITOIRES ET FINALES</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Article 399</td>\n",
              "      <td>Les décisions prononcées avant la date d'entré...</td>\n",
              "      <td>code_famille_2016_full.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1284</th>\n",
              "      <td>code_famille_2016</td>\n",
              "      <td>LIVRE VII: DISPOSITIONS TRANSITOIRES ET FINALES</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Article 400</td>\n",
              "      <td>Pour tout ce qui n'a pas été expressément énon...</td>\n",
              "      <td>code_famille_2016_full.json</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1285 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a20cda2d-58f0-4f3d-9a83-d1ac2139c728')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a20cda2d-58f0-4f3d-9a83-d1ac2139c728 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a20cda2d-58f0-4f3d-9a83-d1ac2139c728');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-31df9d38-e26f-4fe2-8e0c-537615a1f60d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-31df9d38-e26f-4fe2-8e0c-537615a1f60d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-31df9d38-e26f-4fe2-8e0c-537615a1f60d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_8aebd598-e12b-44d6-ab8b-2c7ff52a892b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8aebd598-e12b-44d6-ab8b-2c7ff52a892b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                             code  \\\n",
              "0     constitution_marocaine_2011   \n",
              "1     constitution_marocaine_2011   \n",
              "2     constitution_marocaine_2011   \n",
              "3     constitution_marocaine_2011   \n",
              "4     constitution_marocaine_2011   \n",
              "...                           ...   \n",
              "1280            code_famille_2016   \n",
              "1281            code_famille_2016   \n",
              "1282            code_famille_2016   \n",
              "1283            code_famille_2016   \n",
              "1284            code_famille_2016   \n",
              "\n",
              "                                                livre titre chapitre section  \\\n",
              "0                                           Préambule  None     None    None   \n",
              "1                TITRE PREMIER DISPOSITIONS GENERALES  None     None    None   \n",
              "2                TITRE PREMIER DISPOSITIONS GENERALES  None     None    None   \n",
              "3                TITRE PREMIER DISPOSITIONS GENERALES  None     None    None   \n",
              "4                TITRE PREMIER DISPOSITIONS GENERALES  None     None    None   \n",
              "...                                               ...   ...      ...     ...   \n",
              "1280  LIVRE VII: DISPOSITIONS TRANSITOIRES ET FINALES  None     None    None   \n",
              "1281  LIVRE VII: DISPOSITIONS TRANSITOIRES ET FINALES  None     None    None   \n",
              "1282  LIVRE VII: DISPOSITIONS TRANSITOIRES ET FINALES  None     None    None   \n",
              "1283  LIVRE VII: DISPOSITIONS TRANSITOIRES ET FINALES  None     None    None   \n",
              "1284  LIVRE VII: DISPOSITIONS TRANSITOIRES ET FINALES  None     None    None   \n",
              "\n",
              "           article_no                                               text  \\\n",
              "0           Préambule  Fidèle à son choix irréversible de construire ...   \n",
              "1     Article premier  Le Maroc est une monarchie constitutionnelle, ...   \n",
              "2           Article 2  La souveraineté appartient à la Nation qui l'e...   \n",
              "3           Article 3  L'Islam est la religion de l'Etat, qui garanti...   \n",
              "4           Article 4  L'emblème du Royaume est le drapeau rouge frap...   \n",
              "...               ...                                                ...   \n",
              "1280      Article 396  Les délais prévus par le présent Code sont des...   \n",
              "1281      Article 397  Sont abrogées toutes les dispositions contrair...   \n",
              "1282      Article 398  Demeurent valables, les actes de procédures ef...   \n",
              "1283      Article 399  Les décisions prononcées avant la date d'entré...   \n",
              "1284      Article 400  Pour tout ce qui n'a pas été expressément énon...   \n",
              "\n",
              "                                source_file  \n",
              "0     Constitution_marocaine_2011_full.json  \n",
              "1     Constitution_marocaine_2011_full.json  \n",
              "2     Constitution_marocaine_2011_full.json  \n",
              "3     Constitution_marocaine_2011_full.json  \n",
              "4     Constitution_marocaine_2011_full.json  \n",
              "...                                     ...  \n",
              "1280            code_famille_2016_full.json  \n",
              "1281            code_famille_2016_full.json  \n",
              "1282            code_famille_2016_full.json  \n",
              "1283            code_famille_2016_full.json  \n",
              "1284            code_famille_2016_full.json  \n",
              "\n",
              "[1285 rows x 8 columns]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare the corpus for both retrieval systems with enhanced metadata capture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl9WpLJWmeNh",
        "outputId": "14fcf063-ad84-4dbd-af72-e898b99bf77b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 'text' as the primary text field\n",
            "Found 1285 documents with valid text (out of 1285 total)\n",
            "Prepared corpus with 1275 documents\n",
            "\n",
            "Sample document metadata:\n",
            "  id: doc_0\n",
            "  code: constitution_marocaine_2011\n",
            "  source_file: Constitution_marocaine_2011_full.json\n",
            "  livre: Préambule\n",
            "  article_no: Préambule\n",
            "  article_number: 0\n",
            "  code_display: Constitution Marocaine 2011\n"
          ]
        }
      ],
      "source": [
        "def prepare_corpus(df):\n",
        "    \"\"\"Prepare corpus for retrieval systems by extracting text and generating IDs with complete metadata\"\"\"\n",
        "\n",
        "    # Identify the primary text field ('article', 'text', or 'content')\n",
        "    text_fields = ['article', 'text', 'content']\n",
        "    primary_field = None\n",
        "\n",
        "    for field in text_fields:\n",
        "        if field in df.columns and df[field].notna().sum() > 0:\n",
        "            primary_field = field\n",
        "            break\n",
        "\n",
        "    if primary_field is None:\n",
        "        raise ValueError(\"No suitable text field found in the data\")\n",
        "\n",
        "    print(f\"Using '{primary_field}' as the primary text field\")\n",
        "\n",
        "    # Generate consistent document IDs if they don't exist\n",
        "    if 'id' not in df.columns:\n",
        "        df['id'] = [f\"doc_{i}\" for i in range(len(df))]\n",
        "\n",
        "    # Create corpus list and ID list\n",
        "    corpus = []\n",
        "    doc_ids = []\n",
        "\n",
        "    # Create metadata for dense retrieval\n",
        "    documents = []\n",
        "\n",
        "    # Filter out rows with missing text\n",
        "    valid_rows = df[df[primary_field].notna()]\n",
        "\n",
        "    print(f\"Found {len(valid_rows)} documents with valid text (out of {len(df)} total)\")\n",
        "\n",
        "    # Important metadata fields to preserve specifically\n",
        "    important_fields = [\n",
        "        'article_number', 'article_id', 'title', 'chapter', 'section',\n",
        "        'reference', 'authority', 'description', 'date', 'version',\n",
        "        'jurisdiction'\n",
        "    ]\n",
        "\n",
        "    # Build corpus and IDs\n",
        "    for i, row in valid_rows.iterrows():\n",
        "        # Get text content\n",
        "        text = str(row[primary_field])\n",
        "\n",
        "        # Skip empty texts\n",
        "        if not text or len(text.strip()) < 20:  # Minimum 20 chars to be considered valid\n",
        "            continue\n",
        "\n",
        "        # Add to corpus\n",
        "        corpus.append(text)\n",
        "        doc_ids.append(row['id'])\n",
        "\n",
        "        # Create comprehensive metadata dict\n",
        "        metadata = {\n",
        "            'id': row['id'],\n",
        "            'code': row.get('code', ''),\n",
        "            'source_file': row.get('source_file', '')\n",
        "        }\n",
        "\n",
        "        # Process each column in the dataframe to preserve all metadata\n",
        "        for col in df.columns:\n",
        "            if col not in ['id', 'code', 'source_file', primary_field]:\n",
        "                if col in row and pd.notna(row[col]):\n",
        "                    metadata[col] = row[col]\n",
        "\n",
        "        # Extract article number from filename or id if not already present\n",
        "        if 'article_number' not in metadata:\n",
        "            # Try to extract from source file if available\n",
        "            if 'source_file' in metadata:\n",
        "                import re\n",
        "                file_match = re.search(r'article[_\\-]?(\\d+[\\w\\-\\.]*)', metadata['source_file'], re.IGNORECASE)\n",
        "                if file_match:\n",
        "                    metadata['article_number'] = file_match.group(1)\n",
        "\n",
        "            # Try to extract from document ID\n",
        "            if 'article_number' not in metadata:\n",
        "                id_match = re.search(r'_(\\d+)$', row['id'])\n",
        "                if id_match:\n",
        "                    metadata['article_number'] = id_match.group(1)\n",
        "\n",
        "        # Try to find article number in text content if still not found\n",
        "        if 'article_number' not in metadata:\n",
        "            article_match = re.search(r'(?:Article|Art\\.)\\s+(\\d+[\\w\\-\\.]*)', text[:100])\n",
        "            if article_match:\n",
        "                metadata['article_number'] = article_match.group(1)\n",
        "\n",
        "        # Process code name to make it more readable\n",
        "        if 'code' in metadata and metadata['code']:\n",
        "            metadata['code_display'] = metadata['code'].replace('_', ' ').title()\n",
        "\n",
        "        # Create document for dense retrieval\n",
        "        doc = Document(\n",
        "            text=text,\n",
        "            metadata=metadata\n",
        "        )\n",
        "        documents.append(doc)\n",
        "\n",
        "    print(f\"Prepared corpus with {len(corpus)} documents\")\n",
        "\n",
        "    # Create lookup dictionaries\n",
        "    corpus_lookup = {doc_id: text for doc_id, text in zip(doc_ids, corpus)}\n",
        "\n",
        "    # Print some sample document metadata to verify\n",
        "    if documents:\n",
        "        print(\"\\nSample document metadata:\")\n",
        "        sample = documents[0].metadata\n",
        "        for key, value in sample.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "    return {\n",
        "        \"corpus\": corpus,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"corpus_lookup\": corpus_lookup,\n",
        "        \"documents\": documents\n",
        "    }\n",
        "\n",
        "# Prepare corpus with enhanced metadata\n",
        "corpus_data = prepare_corpus(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OwFXpVP6p31",
        "outputId": "954afc14-e51a-4769-95f3-e992dd994842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus lookup saved successfully\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(\"D:/a_PROJECTS/legal-rag-assistant/Models\", exist_ok=True)\n",
        "\n",
        "# Save the corpus_data dictionary to a pickle file\n",
        "with open(\"/content/drive/MyDrive/legal-rag-assistant/knowledge_base/vector_store/corpus_lookup.pkl\", \"wb\") as f:\n",
        "    pickle.dump(corpus_data, f)\n",
        "\n",
        "print(\"Corpus lookup saved successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create BM25 Plus Sparse Retrieval Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cmA7_UY0ns8v"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text, language='french'):\n",
        "    \"\"\"Preprocess text for sparse retrieval\"\"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text, language=language)\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    french_stopwords = set(stopwords.words(language))\n",
        "    tokens = [token for token in tokens if token.isalnum() and token not in french_stopwords]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "class BM25PlusRetriever:\n",
        "    \"\"\"BM25 Plus based retrieval model.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.bm25 = None\n",
        "        self.tokenized_corpus = None\n",
        "        self.doc_ids = None\n",
        "\n",
        "    def fit(self, corpus, doc_ids):\n",
        "        \"\"\"Build the BM25 index.\"\"\"\n",
        "        print(\"Tokenizing corpus for BM25 Plus...\")\n",
        "        self.tokenized_corpus = [preprocess_text(doc) for doc in tqdm(corpus)]\n",
        "        self.doc_ids = doc_ids\n",
        "\n",
        "        print(\"Building BM25 Plus index...\")\n",
        "        self.bm25 = BM25Plus(self.tokenized_corpus)\n",
        "        print(\"BM25 Plus index built successfully\")\n",
        "\n",
        "    def retrieve(self, query, top_k=5):\n",
        "        \"\"\"Retrieve top-k relevant documents.\"\"\"\n",
        "        query_tokens = preprocess_text(query)\n",
        "        scores = self.bm25.get_scores(query_tokens)\n",
        "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
        "\n",
        "        results = [(self.doc_ids[idx], scores[idx]) for idx in top_indices]\n",
        "        return results\n",
        "\n",
        "    def save(self, path):\n",
        "        \"\"\"Save the model to disk.\"\"\"\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'bm25': self.bm25,\n",
        "                'doc_ids': self.doc_ids\n",
        "            }, f)\n",
        "        print(f\"BM25 Plus model saved to {path}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        \"\"\"Load the model from disk.\"\"\"\n",
        "        with open(path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            model = cls()\n",
        "            model.bm25 = data['bm25']\n",
        "            model.doc_ids = data['doc_ids']\n",
        "            print(f\"BM25 Plus model loaded from {path}\")\n",
        "            return model\n",
        "\n",
        "## Create sparse model directory\n",
        "# sparse_model_dir = '/mnt/d/a_PROJECTS/legal-rag-assistant/knowledge_base/vector_store/sparse'\n",
        "# os.makedirs(sparse_model_dir, exist_ok=True)\n",
        "\n",
        "# # Build or load BM25 Plus model\n",
        "# bm25_plus_path = f\"{sparse_model_dir}/bm25_plus.pkl\"\n",
        "# if os.path.exists(bm25_plus_path):\n",
        "#     print(\"Loading existing BM25 Plus model...\")\n",
        "#     sparse_model = BM25PlusRetriever.load(bm25_plus_path)\n",
        "# else:\n",
        "#     print(\"Building new BM25 Plus model...\")\n",
        "#     sparse_model = BM25PlusRetriever()\n",
        "#     sparse_model.fit(corpus_data['corpus'], corpus_data['doc_ids'])\n",
        "#     sparse_model.save(bm25_plus_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create LlamaIndex Dense Retrieval Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LIczAVM3qUB6"
      },
      "outputs": [],
      "source": [
        "class DenseRetriever:\n",
        "    \"\"\"Dense retrieval model using LlamaIndex.\"\"\"\n",
        "    def __init__(self, embed_model_name=\"intfloat/multilingual-e5-large\"):\n",
        "        \"\"\"Initialize with a multilingual embedding model that works well for French\"\"\"\n",
        "        # Check if GPU is available\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"Using device: {device} for embeddings\")\n",
        "\n",
        "        self.embed_model = HuggingFaceEmbedding(\n",
        "            model_name=embed_model_name,\n",
        "            device=device\n",
        "        )\n",
        "        Settings.embed_model = self.embed_model\n",
        "        self.index = None\n",
        "        self.doc_ids = None\n",
        "\n",
        "    def fit(self, documents):\n",
        "        \"\"\"Build the vector index from documents.\"\"\"\n",
        "        print(\"Building dense vector index...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Store doc IDs for retrieval\n",
        "        self.doc_ids = [doc.metadata['id'] for doc in documents]\n",
        "\n",
        "        # Build index\n",
        "        vector_store = SimpleVectorStore()\n",
        "        self.index = VectorStoreIndex.from_documents(\n",
        "            documents,\n",
        "            vector_store=vector_store,\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "        print(f\"Vector index built in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    def retrieve(self, query, top_k=5):\n",
        "        \"\"\"Retrieve top-k relevant documents.\"\"\"\n",
        "        retriever = self.index.as_retriever(similarity_top_k=top_k)\n",
        "        results = retriever.retrieve(query)\n",
        "\n",
        "        retrieved_docs = []\n",
        "        for node in results:\n",
        "            doc_id = node.metadata[\"id\"]\n",
        "            score = node.score if hasattr(node, \"score\") else 0.0\n",
        "            retrieved_docs.append((doc_id, score))\n",
        "\n",
        "        return retrieved_docs\n",
        "\n",
        "    def save(self, path):\n",
        "        \"\"\"Save the index to disk.\"\"\"\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        self.index.storage_context.persist(persist_dir=path)\n",
        "\n",
        "        # Save doc_ids separately since they're not stored in the index\n",
        "        with open(os.path.join(path, \"doc_ids.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(self.doc_ids, f)\n",
        "\n",
        "        print(f\"Dense vector index saved to {path}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path, embed_model_name=\"intfloat/multilingual-e5-large\"):\n",
        "        \"\"\"Load the index from disk.\"\"\"\n",
        "        from llama_index.core import load_index_from_storage\n",
        "        from llama_index.core import StorageContext\n",
        "\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(f\"Path not found: {path}\")\n",
        "\n",
        "        model = cls(embed_model_name=embed_model_name)\n",
        "        storage_context = StorageContext.from_defaults(persist_dir=path)\n",
        "        model.index = load_index_from_storage(storage_context)\n",
        "\n",
        "        # Load doc_ids\n",
        "        with open(os.path.join(path, \"doc_ids.pkl\"), \"rb\") as f:\n",
        "            model.doc_ids = pickle.load(f)\n",
        "\n",
        "        print(f\"Dense vector index loaded from {path}\")\n",
        "        return model\n",
        "\n",
        "# # Create dense model directory\n",
        "# dense_model_dir = '/mnt/d/a_PROJECTS/legal-rag-assistant/knowledge_base/vector_store/dense'\n",
        "# dense_index_path = f\"{dense_model_dir}/legal_dense_index\"\n",
        "\n",
        "# # Build or load Dense model\n",
        "# if os.path.exists(dense_index_path):\n",
        "#     print(\"Loading existing Dense vector index...\")\n",
        "#     dense_model = DenseRetriever.load(dense_index_path)\n",
        "# else:\n",
        "#     print(\"Building new Dense vector index...\")\n",
        "#     dense_model = DenseRetriever()\n",
        "#     dense_model.fit(corpus_data['documents'])\n",
        "#     dense_model.save(dense_index_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Reciprocal Rank Fusion Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kYDa2J6ZqrNf"
      },
      "outputs": [],
      "source": [
        "class ReciprocalRankFusionRetriever:\n",
        "    \"\"\"Implements Reciprocal Rank Fusion for combining multiple retrieval methods.\"\"\"\n",
        "\n",
        "    def __init__(self, retrievers, k=20):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            retrievers: List of retriever models\n",
        "            k: Constant to prevent items with very low ranks from having too much influence\n",
        "        \"\"\"\n",
        "        self.retrievers = retrievers\n",
        "        self.k = k\n",
        "        self.name = f\"RRF(k={k})\"\n",
        "\n",
        "    def retrieve(self, query, top_k=5, per_retriever_k=50):\n",
        "        \"\"\"Retrieve documents using RRF ranking.\"\"\"\n",
        "        # Get results from all retrievers\n",
        "        all_results = []\n",
        "        for retriever in self.retrievers:\n",
        "            results = retriever.retrieve(query, top_k=per_retriever_k)\n",
        "            all_results.append(results)\n",
        "\n",
        "        # Calculate RRF scores\n",
        "        rrf_scores = {}\n",
        "\n",
        "        for result_set in all_results:\n",
        "            for rank, (doc_id, _) in enumerate(result_set):\n",
        "                if doc_id not in rrf_scores:\n",
        "                    rrf_scores[doc_id] = 0\n",
        "                # RRF formula: 1 / (k + rank)\n",
        "                rrf_scores[doc_id] += 1 / (self.k + rank + 1)  # +1 because rank is 0-indexed\n",
        "\n",
        "        # Sort by RRF score and return top-k\n",
        "        sorted_results = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "        return sorted_results[:top_k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Initialise and save the hybrid strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2UF7PIhvqDg"
      },
      "outputs": [],
      "source": [
        "best_hybrid = ReciprocalRankFusionRetriever([sparse_model, dense_model], k=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8xfF8gG5LeE",
        "outputId": "63a0f496-2624-4116-cb8f-8f06bb009226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved hybrid retriever configurations\n"
          ]
        }
      ],
      "source": [
        "# Let's save the hybrid retrievers\n",
        "hybrid_model_dir = '/content/drive/MyDrive/legal-rag-assistant/hybrid-retrieval'\n",
        "os.makedirs(hybrid_model_dir, exist_ok=True)\n",
        "\n",
        "# Save configuration for hybrid models\n",
        "hybrid_config = {\n",
        "    'rrf': {\n",
        "        'type': 'rrf',\n",
        "        'k': 20\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f\"{hybrid_model_dir}/hybrid_config.json\", 'w') as f:\n",
        "    json.dump(hybrid_config, f, indent=2)\n",
        "\n",
        "print(\"Saved hybrid retriever configurations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Test the hybrid retrieval strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKZwCCUJv16X",
        "outputId": "638074a5-bc5e-491b-9676-a305d09b5ba8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "QUERY: Comment fonctionne la procédure de divorce par consentement mutuel?\n",
            "================================================================================\n",
            "\n",
            "Retrieved 3 documents in 0.11 seconds\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DOCUMENT 1 | Score: 0.0911 | Article 1007 | Code Famille 2016\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE INFORMATION:\n",
            "• Legal Code:        Code Famille 2016\n",
            "• Article Number:    1007\n",
            "• Source File:       code_famille_2016_full.json\n",
            "• Document ID:       doc_1007\n",
            "\n",
            "ADDITIONAL METADATA:\n",
            "• Livre:             LIVRE II: DE LA DISSOLUTION DU PACTE DE MARIAGE ET DE SES EFFETS\n",
            "• Titre:             TITRE VI: DES CATEGORIES DE DIVORCE SOUS CONTROLE JUDICIAIRE ET DE DIVORCE JUDICIAIRE\n",
            "• Chapitre:          CHAPITRE II: DU DIVORCE RÉVOCABLE (RIJII) ET DU DIVORCE IRRÉVOCABLE (BAIN)\n",
            "• Article No:        Article 123\n",
            "\n",
            "DOCUMENT TEXT:\n",
            "Tout divorce du fait de l'époux est révocable, à l'exception du divorce\n",
            "prononcé à la suite de deux précédents divorces successifs, du divorce\n",
            "intervenu avant la consommation du mariage, du divorce par\n",
            "consentement mutuel, du divorce par Khol et de celui qui résulte d'un\n",
            "droit d'option consenti par l'époux à son épouse.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DOCUMENT 2 | Score: 0.0819 | Article 894 | Code Famille 2016\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE INFORMATION:\n",
            "• Legal Code:        Code Famille 2016\n",
            "• Article Number:    894\n",
            "• Source File:       code_famille_2016_full.json\n",
            "• Document ID:       doc_894\n",
            "\n",
            "ADDITIONAL METADATA:\n",
            "• Livre:             LIVRE PREMIER: DU MARIAGE\n",
            "• Titre:             TITRE PREMIER: DES FIANÇAILLES ET DU MARIAGE\n",
            "• Chapitre:          CHAPITRE II: DU MARIAGE\n",
            "• Article No:        Article 10\n",
            "\n",
            "DOCUMENT TEXT:\n",
            "Le mariage est conclu par consentement mutuel ( Ijab et Quaboul) des\n",
            "deux contractants, exprimé en termes consacrés ou à l'aide de toute\n",
            "expression admise par la langue ou l'usage.\n",
            "Pour toute personne se trouvant dans l'incapacité de s'exprimer\n",
            "oralement, le consentement résulte valablement d'un écrit si l'intéressé\n",
            "peut écrire, sinon d'un signe compréhensible par l'autre partie et par les\n",
            "deux adoul.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DOCUMENT 3 | Score: 0.0725 | Article 1000 | Code Famille 2016\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE INFORMATION:\n",
            "• Legal Code:        Code Famille 2016\n",
            "• Article Number:    1000\n",
            "• Source File:       code_famille_2016_full.json\n",
            "• Document ID:       doc_1000\n",
            "\n",
            "ADDITIONAL METADATA:\n",
            "• Livre:             LIVRE II: DE LA DISSOLUTION DU PACTE DE MARIAGE ET DE SES EFFETS\n",
            "• Titre:             TITRE V: DU DIVORCE PAR CONSENTEMENT MUTUEL OU MOYENNANT COMPENSATION (KHOL)\n",
            "• Chapitre:          CHAPITRE II: DU DIVORCE PAR KHOL\n",
            "• Article No:        Article 116\n",
            "\n",
            "DOCUMENT TEXT:\n",
            "Le consentement d'une femme majeure à la compensation en vue\n",
            "d'obtenir son divorce par Khol est valable. Si le consentement émane\n",
            "d'une femme mineure, le divorce est acquis et la mineure n'est tenue à la\n",
            "compensation qu'avec l'accord de son représentant légal.\n",
            "\n",
            "================================================================================\n",
            "CITATIONS:\n",
            "[1] Code Famille 2016 — Article 1007\n",
            "[2] Code Famille 2016 — Article 894\n",
            "[3] Code Famille 2016 — Article 1000\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def retrieve_and_display_legal_documents(query, top_k=3, include_text=True, max_text_length=500):\n",
        "    \"\"\"\n",
        "    Retrieve the top k documents for a legal question using the best hybrid retriever\n",
        "    and display them in an organized way with complete metadata.\n",
        "\n",
        "    Args:\n",
        "        query: User's legal question\n",
        "        top_k: Number of documents to retrieve\n",
        "        include_text: Whether to include the document text in the display\n",
        "        max_text_length: Maximum length of displayed text (if include_text is True)\n",
        "\n",
        "    Returns:\n",
        "        Retrieved documents (for further processing if needed)\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"QUERY: {query}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    # Retrieve documents using the best hybrid retriever\n",
        "    start_time = time.time()\n",
        "    results = best_hybrid.retrieve(query, top_k=top_k)\n",
        "    retrieval_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Retrieved {len(results)} documents in {retrieval_time:.2f} seconds\\n\")\n",
        "\n",
        "    # Process and display each document\n",
        "    retrieved_docs = []\n",
        "    for i, (doc_id, score) in enumerate(results):\n",
        "        # Get document text\n",
        "        document_text = corpus_data['corpus_lookup'].get(doc_id, \"Document text not found\")\n",
        "\n",
        "        # Get document metadata\n",
        "        doc_idx = corpus_data['doc_ids'].index(doc_id) if doc_id in corpus_data['doc_ids'] else -1\n",
        "\n",
        "        if doc_idx >= 0 and doc_idx < len(corpus_data['documents']):\n",
        "            metadata = corpus_data['documents'][doc_idx].metadata\n",
        "        else:\n",
        "            metadata = {'id': doc_id}\n",
        "\n",
        "        # Save document info\n",
        "        retrieved_docs.append({\n",
        "            'id': doc_id,\n",
        "            'text': document_text,\n",
        "            'score': score,\n",
        "            'metadata': metadata\n",
        "        })\n",
        "\n",
        "        # Get formatted code name for display\n",
        "        if 'code_display' in metadata:\n",
        "            code_name = metadata['code_display']\n",
        "        elif 'code' in metadata:\n",
        "            code_name = metadata['code'].replace('_', ' ').title()\n",
        "        else:\n",
        "            code_name = \"Unknown Code\"\n",
        "\n",
        "        # Get article number for header\n",
        "        article_info = \"\"\n",
        "        if 'article_number' in metadata:\n",
        "            article_info = f\"Article {metadata['article_number']} | \"\n",
        "        elif 'article_id' in metadata:\n",
        "            article_info = f\"Article {metadata['article_id']} | \"\n",
        "\n",
        "        # Display document header with core info\n",
        "        print(f\"\\n{'-'*80}\")\n",
        "        print(f\"DOCUMENT {i+1} | Score: {score:.4f} | {article_info}{code_name}\")\n",
        "        print(f\"{'-'*80}\")\n",
        "\n",
        "        # Display the most important metadata fields first\n",
        "        print(\"SOURCE INFORMATION:\")\n",
        "\n",
        "        # Display legal code (always important)\n",
        "        if 'code' in metadata:\n",
        "            display_code = metadata.get('code_display', metadata['code'].replace('_', ' ').title())\n",
        "            print(f\"• Legal Code:        {display_code}\")\n",
        "\n",
        "        # Display article number or reference (key identifiers)\n",
        "        article_shown = False\n",
        "        if 'article_number' in metadata:\n",
        "            print(f\"• Article Number:    {metadata['article_number']}\")\n",
        "            article_shown = True\n",
        "        if 'article_id' in metadata and not article_shown:\n",
        "            print(f\"• Article ID:        {metadata['article_id']}\")\n",
        "            article_shown = True\n",
        "        if 'reference' in metadata and not article_shown:\n",
        "            print(f\"• Reference:         {metadata['reference']}\")\n",
        "\n",
        "        # Display section/chapter info if available\n",
        "        if 'section' in metadata:\n",
        "            print(f\"• Section:           {metadata['section']}\")\n",
        "        if 'chapter' in metadata:\n",
        "            print(f\"• Chapter:           {metadata['chapter']}\")\n",
        "        if 'title' in metadata:\n",
        "            print(f\"• Title:             {metadata['title']}\")\n",
        "\n",
        "        # Display authority/jurisdiction\n",
        "        if 'authority' in metadata:\n",
        "            print(f\"• Authority:         {metadata['authority']}\")\n",
        "        if 'jurisdiction' in metadata:\n",
        "            print(f\"• Jurisdiction:      {metadata['jurisdiction']}\")\n",
        "\n",
        "        # Display version/date info\n",
        "        if 'version' in metadata:\n",
        "            print(f\"• Version:           {metadata['version']}\")\n",
        "        if 'date' in metadata:\n",
        "            print(f\"• Date:              {metadata['date']}\")\n",
        "\n",
        "        # Display description if available\n",
        "        if 'description' in metadata:\n",
        "            desc = metadata['description']\n",
        "            if len(desc) > 100:\n",
        "                desc = desc[:97] + \"...\"\n",
        "            print(f\"• Description:       {desc}\")\n",
        "\n",
        "        # Display source file info\n",
        "        if 'source_file' in metadata:\n",
        "            print(f\"• Source File:       {metadata['source_file']}\")\n",
        "\n",
        "        # Display document ID as fallback or for reference\n",
        "        print(f\"• Document ID:       {doc_id}\")\n",
        "\n",
        "        # Display any remaining metadata fields\n",
        "        excluded_fields = [\n",
        "            'id', 'code', 'code_display', 'article_number', 'article_id', 'reference',\n",
        "            'section', 'chapter', 'title', 'authority', 'jurisdiction',\n",
        "            'version', 'date', 'description', 'source_file'\n",
        "        ]\n",
        "\n",
        "        other_fields = [k for k in metadata.keys() if k not in excluded_fields]\n",
        "\n",
        "        if other_fields:\n",
        "            print(\"\\nADDITIONAL METADATA:\")\n",
        "            for field in other_fields:\n",
        "                value = metadata[field]\n",
        "                # Truncate long values\n",
        "                if isinstance(value, str) and len(value) > 100:\n",
        "                    value = value[:97] + \"...\"\n",
        "                print(f\"• {field.replace('_', ' ').title()}:\".ljust(20) + f\" {value}\")\n",
        "\n",
        "        # Display document text if requested\n",
        "        if include_text:\n",
        "            print(\"\\nDOCUMENT TEXT:\")\n",
        "            if len(document_text) > max_text_length:\n",
        "                display_text = document_text[:max_text_length] + \"...\"\n",
        "            else:\n",
        "                display_text = document_text\n",
        "            print(f\"{display_text}\")\n",
        "\n",
        "    # Display citation format for all documents\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"CITATIONS:\")\n",
        "    for i, doc in enumerate(retrieved_docs):\n",
        "        citation = format_legal_citation(doc['metadata'])\n",
        "        print(f\"[{i+1}] {citation}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    return retrieved_docs\n",
        "\n",
        "def format_legal_citation(metadata):\n",
        "    \"\"\"\n",
        "    Format metadata into a proper legal citation string.\n",
        "    Formats citations in a standardized format depending on available metadata.\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "\n",
        "    # Add code/law name if available with nice formatting\n",
        "    if 'code_display' in metadata:\n",
        "        parts.append(metadata['code_display'])\n",
        "    elif 'code' in metadata:\n",
        "        parts.append(metadata['code'].replace('_', ' ').title())\n",
        "\n",
        "    # Add article reference with proper formatting\n",
        "    if 'article_number' in metadata:\n",
        "        parts.append(f\"Article {metadata['article_number']}\")\n",
        "    elif 'article_id' in metadata:\n",
        "        parts.append(f\"Article {metadata['article_id']}\")\n",
        "    elif 'reference' in metadata:\n",
        "        parts.append(f\"{metadata['reference']}\")\n",
        "\n",
        "    # Add section/chapter if available\n",
        "    section_info = []\n",
        "    if 'section' in metadata:\n",
        "        section_info.append(f\"Section {metadata['section']}\")\n",
        "    if 'chapter' in metadata:\n",
        "        section_info.append(f\"Chapter {metadata['chapter']}\")\n",
        "\n",
        "    if section_info:\n",
        "        parts.append(\", \".join(section_info))\n",
        "\n",
        "    # Add authority/jurisdiction\n",
        "    if 'authority' in metadata:\n",
        "        parts.append(f\"{metadata['authority']}\")\n",
        "\n",
        "    # Add date/version if available\n",
        "    if 'date' in metadata:\n",
        "        parts.append(f\"({metadata['date']})\")\n",
        "    elif 'version' in metadata:\n",
        "        parts.append(f\"({metadata['version']})\")\n",
        "\n",
        "    # If we still don't have any parts, use ID as fallback\n",
        "    if not parts and 'id' in metadata:\n",
        "        parts.append(f\"Document ID: {metadata['id']}\")\n",
        "\n",
        "    return \" — \".join(parts)\n",
        "\n",
        "# Example usage\n",
        "test_query = \"Comment fonctionne la procédure de divorce par consentement mutuel?\"\n",
        "retrieved_docs = retrieve_and_display_legal_documents(test_query, top_k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. RAG legal Piepline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DW2yPM3K7PWS",
        "outputId": "04955d7f-22b8-4680-add0-57c1e9017a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting vllm\n",
            "  Downloading vllm-0.8.5.post1-cp38-abi3-manylinux1_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vllm) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\n",
            "Collecting blake3 (from vllm)\n",
            "  Downloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.51.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.51.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.30.0->vllm) (0.30.2)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (5.29.4)\n",
            "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.15)\n",
            "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.76.2)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.4)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.2.1)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\n",
            "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n",
            "  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llguidance<0.8.0,>=0.7.9 (from vllm)\n",
            "  Downloading llguidance-0.7.19-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting outlines==0.1.11 (from vllm)\n",
            "  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lark==1.2.2 (from vllm)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar==0.1.18 (from vllm)\n",
            "  Downloading xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.13.2)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.18.0)\n",
            "Collecting partial-json-parser (from vllm)\n",
            "  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting pyzmq>=25.0.0 (from vllm)\n",
            "  Downloading pyzmq-26.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting msgspec (from vllm)\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf>=0.13.0 (from vllm)\n",
            "  Downloading gguf-0.16.3-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm) (8.7.0)\n",
            "Collecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n",
            "  Downloading mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\n",
            "Collecting compressed-tensors==0.9.3 (from vllm)\n",
            "  Downloading compressed_tensors-0.9.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting depyf==0.18.0 (from vllm)\n",
            "  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\n",
            "Collecting watchfiles (from vllm)\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting python-json-logger (from vllm)\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.15.2)\n",
            "Collecting ninja (from vllm)\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting opentelemetry-sdk<1.27.0,>=1.26.0 (from vllm)\n",
            "  Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api<1.27.0,>=1.26.0 (from vllm)\n",
            "  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp<1.27.0,>=1.26.0 (from vllm)\n",
            "  Downloading opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions-ai<0.5.0,>=0.4.1 (from vllm)\n",
            "  Downloading opentelemetry_semantic_conventions_ai-0.4.8-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting numba==0.61.2 (from vllm)\n",
            "  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
            "  Downloading ray-2.46.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision==0.21.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.0+cu124)\n",
            "Collecting xformers==0.0.29.post2 (from vllm)\n",
            "  Downloading xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting astor (from depyf==0.18.0->vllm)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting dill (from depyf==0.18.0->vllm)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n",
            "  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting interegular (from outlines==0.1.11->vllm)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
            "Collecting diskcache (from outlines==0.1.11->vllm)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
            "Collecting pycountry (from outlines==0.1.11->vllm)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting airportsdata (from outlines==0.1.11->vllm)\n",
            "  Downloading airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n",
            "  Downloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->vllm) (1.3.0)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
            "Collecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.0->huggingface-hub[hf_xet]>=0.30.0->vllm) (24.2)\n",
            "Collecting hf-xet>=0.1.4 (from huggingface-hub[hf_xet]>=0.30.0->vllm)\n",
            "  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<1.27.0,>=1.26.0->vllm) (1.2.18)\n",
            "Collecting importlib_metadata (from vllm)\n",
            "  Downloading importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->vllm) (3.21.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.26.0 (from opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm) (1.71.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc==1.26.0->opentelemetry-exporter-otlp<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting protobuf (from vllm)\n",
            "  Downloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-sdk<1.27.0,>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.1.8)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (13.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2025.4.26)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.1->vllm) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.20.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<1.27.0,>=1.26.0->vllm) (1.17.2)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.3)\n",
            "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading rich_toolkit-0.14.5-py3-none-any.whl.metadata (999 bytes)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2025.4.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.24.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.8.3)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
            "Downloading vllm-0.8.5.post1-cp38-abi3-manylinux1_x86_64.whl (326.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.4/326.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.9.3-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.4/98.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl (44.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgrammar-0.1.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.16.3-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.4/94.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llguidance-0.7.19-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.5.4-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\n",
            "Downloading opentelemetry_exporter_otlp-1.26.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions_ai-0.4.8-py3-none-any.whl (5.6 kB)\n",
            "Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyzmq-26.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (862 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.4/862.4 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.46.0-cp311-cp311-manylinux2014_x86_64.whl (68.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blake3-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
            "Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
            "Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading airportsdata-20250224-py3-none-any.whl (913 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_toolkit-0.14.5-py3-none-any.whl (24 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: blake3, uvloop, uvicorn, pyzmq, python-multipart, python-json-logger, pycountry, protobuf, partial-json-parser, opentelemetry-semantic-conventions-ai, ninja, msgspec, llvmlite, llguidance, lark, interegular, importlib_metadata, httptools, hf-xet, gguf, dnspython, diskcache, dill, astor, airportsdata, watchfiles, starlette, opentelemetry-proto, opentelemetry-api, numba, email-validator, depyf, rich-toolkit, prometheus-fastapi-instrumentator, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, lm-format-enforcer, fastapi, xformers, ray, outlines_core, opentelemetry-sdk, mistral_common, fastapi-cli, xgrammar, outlines, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, compressed-tensors, opentelemetry-exporter-otlp, vllm\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 24.0.1\n",
            "    Uninstalling pyzmq-24.0.1:\n",
            "      Successfully uninstalled pyzmq-24.0.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib_metadata 8.7.0\n",
            "    Uninstalling importlib_metadata-8.7.0:\n",
            "      Successfully uninstalled importlib_metadata-8.7.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.16.0\n",
            "    Uninstalling opentelemetry-api-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.16.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.16.0\n",
            "    Uninstalling opentelemetry-sdk-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed airportsdata-20250224 astor-0.8.1 blake3-1.0.4 compressed-tensors-0.9.3 depyf-0.18.0 dill-0.4.0 diskcache-5.6.3 dnspython-2.7.0 email-validator-2.2.0 fastapi-0.115.12 fastapi-cli-0.0.7 gguf-0.16.3 hf-xet-1.1.0 httptools-0.6.4 importlib_metadata-8.0.0 interegular-0.3.3 lark-1.2.2 llguidance-0.7.19 llvmlite-0.44.0 lm-format-enforcer-0.10.11 mistral_common-1.5.4 msgspec-0.19.0 ninja-1.11.1.4 numba-0.61.2 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-exporter-otlp-proto-http-1.26.0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 opentelemetry-semantic-conventions-ai-0.4.8 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.1.0 protobuf-4.25.7 pycountry-24.6.1 python-json-logger-3.3.0 python-multipart-0.0.20 pyzmq-26.4.0 ray-2.46.0 rich-toolkit-0.14.5 starlette-0.46.2 uvicorn-0.34.2 uvloop-0.21.0 vllm-0.8.5.post1 watchfiles-1.0.5 xformers-0.0.29.post2 xgrammar-0.1.18\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "365f10a1e0ff465cb213083eb6db1e6b",
              "pip_warning": {
                "packages": [
                  "importlib_metadata"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pip install vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /home/youssef/vllm_env/lib/python3.11/site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /home/youssef/vllm_env/lib/python3.11/site-packages (from pandas) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/youssef/vllm_env/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/youssef/vllm_env/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/youssef/vllm_env/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/youssef/vllm_env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "import pickle\n",
        "\n",
        "# # For hybrid retrieval\n",
        "# from Models.sparse.bm25_retriever import BM25PlusRetriever\n",
        "# from Models.dense.dense_retriever import DenseRetriever\n",
        "# from Models.hybrid.hybrid_retriever import ReciprocalRankFusionRetriever\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 05-13 00:04:04 [importing.py:53] Triton module has been replaced with a placeholder.\n",
            "INFO 05-13 00:04:04 [__init__.py:239] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "# For VLLM inference\n",
        "from vllm import LLM, SamplingParams\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tN2S2rsp5qJt",
        "outputId": "ff741321-a449-4fb5-b68f-5d6489619c75"
      },
      "outputs": [],
      "source": [
        "class LegalRAGPipeline:\n",
        "    \"\"\"\n",
        "    End-to-end RAG pipeline for legal question answering using:\n",
        "    - Hybrid retrieval (BM25 + Dense)\n",
        "    - Fine-tuned Qwen1.5 model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str = \"/mnt/d/a_PROJECTS/legal-rag-assistant/FineTuned-Qwen2/qwen-legal-assistant/merged_16bit\",\n",
        "        sparse_model_path: str = \"/mnt/d/a_PROJECTS/legal-rag-assistant/knowledge_base/vector_store/sparse/bm25_plus.pkl\",\n",
        "        dense_model_path: str = \"/mnt/d/a_PROJECTS/legal-rag-assistant/knowledge_base/vector_store/dense/legal_dense_index\",\n",
        "        hybrid_config_path: str = \"/mnt/d/a_PROJECTS/legal-rag-assistant/hybrid-retrieval/hybrid_config.json\",\n",
        "        corpus_lookup_path: str = \"/mnt/d/a_PROJECTS/legal-rag-assistant/knowledge_base/vector_store/corpus_lookup.pkl\",\n",
        "        max_gpu_memory: float = 0.85,\n",
        "        top_k: int = 3\n",
        "    ):\n",
        "        \"\"\"Initialize the legal RAG pipeline with the specified models and parameters\"\"\"\n",
        "        self.top_k = top_k\n",
        "\n",
        "        print(\"Initializing Legal RAG Pipeline...\")\n",
        "\n",
        "        # Step 1: Load the lookup dictionary for document retrieval\n",
        "        print(\"Loading corpus lookup...\")\n",
        "        import pickle\n",
        "        with open(corpus_lookup_path, 'rb') as f:\n",
        "            self.corpus_data = pickle.load(f)\n",
        "\n",
        "        print(f\"Loaded corpus with {len(self.corpus_data['doc_ids'])} documents\")\n",
        "\n",
        "        # Step 2: Load the retrieval models\n",
        "        print(\"Loading retrieval models...\")\n",
        "        self._load_retrieval_models(sparse_model_path, dense_model_path, hybrid_config_path)\n",
        "\n",
        "        # Step 3: Load the LLM for generation\n",
        "        print(\"Loading Qwen model with VLLM...\")\n",
        "        self._load_llm(model_path, max_gpu_memory)\n",
        "\n",
        "        print(\"Legal RAG Pipeline initialized successfully!\")\n",
        "\n",
        "    def _load_retrieval_models(self, sparse_model_path, dense_model_path, hybrid_config_path):\n",
        "        \"\"\"Load and configure retrieval models\"\"\"\n",
        "        # Load BM25Plus retriever\n",
        "        print(\"Loading BM25+ retriever...\")\n",
        "        self.sparse_model = BM25PlusRetriever.load(sparse_model_path)\n",
        "\n",
        "        # Load Dense retriever\n",
        "        print(\"Loading dense retriever...\")\n",
        "        self.dense_model = DenseRetriever.load(dense_model_path)\n",
        "\n",
        "        # Load hybrid configuration\n",
        "        with open(hybrid_config_path, 'r') as f:\n",
        "            hybrid_config = json.load(f)\n",
        "\n",
        "        # Create RRF hybrid retriever with configured k parameter\n",
        "        self.hybrid_retriever = ReciprocalRankFusionRetriever(\n",
        "            [self.sparse_model, self.dense_model],\n",
        "            k=hybrid_config.get('rrf_k', 60)\n",
        "        )\n",
        "\n",
        "    def _load_llm(self, model_path, max_gpu_memory):\n",
        "        \"\"\"Load the LLM model with VLLM for efficient inference\"\"\"\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "        # Check for GPU and determine appropriate params\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_name = torch.cuda.get_device_name(0)\n",
        "            print(f\"Using GPU: {gpu_name}\")\n",
        "\n",
        "            # Lower context window on lower-end GPUs\n",
        "            max_model_len = 4096\n",
        "\n",
        "            # Load model with VLLM\n",
        "            self.llm = LLM(\n",
        "                model=model_path,\n",
        "                tensor_parallel_size=1,  # Using single GPU\n",
        "                gpu_memory_utilization=max_gpu_memory,\n",
        "                max_model_len=max_model_len,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "        else:\n",
        "            print(\"WARNING: No GPU detected. This will be extremely slow.\")\n",
        "            self.llm = LLM(\n",
        "                model=model_path,\n",
        "                tensor_parallel_size=1,\n",
        "                max_model_len=2048,  # Conservative for CPU\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "\n",
        "    def _create_legal_system_prompt(self):\n",
        "        \"\"\"Create the system prompt for the legal assistant\"\"\"\n",
        "        return \"\"\"You are LegalAssistant, a professional legal advisor specializing in Moroccan law.\n",
        "\n",
        "When answering questions:\n",
        "- Base your answers strictly on the provided legal context\n",
        "- Cite specific articles mentioned in the context by code name and article number\n",
        "- If information is insufficient, state clearly \"Based on the provided context, I don't have enough information to answer this question completely\" rather than guessing\n",
        "- Be concise and direct, avoiding unnecessary elaboration\n",
        "- Use clear language that non-lawyers can understand\n",
        "- Structure complex answers with numbered points for clarity\n",
        "- Maintain a professional, helpful tone throughout\n",
        "\n",
        "Your goal is to provide accurate legal information without hallucination or speculation.\"\"\"\n",
        "\n",
        "    def retrieve_documents(self, query: str) -> List[Dict]:\n",
        "        \"\"\"Retrieve relevant documents using hybrid retrieval\"\"\"\n",
        "        # Get document IDs and scores from the hybrid retriever\n",
        "        results = self.hybrid_retriever.retrieve(query, top_k=self.top_k)\n",
        "\n",
        "        # Format results\n",
        "        documents = []\n",
        "        for doc_id, score in results:\n",
        "            document_text = self.corpus_data['corpus_lookup'].get(doc_id, \"\")\n",
        "\n",
        "            # Find the original document to get metadata\n",
        "            doc_idx = self.corpus_data['doc_ids'].index(doc_id) if doc_id in self.corpus_data['doc_ids'] else -1\n",
        "\n",
        "            if doc_idx >= 0 and doc_idx < len(self.corpus_data['documents']):\n",
        "                metadata = self.corpus_data['documents'][doc_idx].metadata\n",
        "            else:\n",
        "                metadata = {'id': doc_id}\n",
        "\n",
        "            documents.append({\n",
        "                'id': doc_id,\n",
        "                'text': document_text,\n",
        "                'score': score,\n",
        "                'metadata': metadata\n",
        "            })\n",
        "\n",
        "        return documents\n",
        "\n",
        "    def format_context(self, documents: List[Dict]) -> str:\n",
        "        \"\"\"Format retrieved documents into a context string for the LLM\"\"\"\n",
        "        context_parts = []\n",
        "\n",
        "        for i, doc in enumerate(documents):\n",
        "            # Format article reference\n",
        "            article_ref = self._format_article_reference(doc['metadata'])\n",
        "\n",
        "            # Add to context with clear separation\n",
        "            context_parts.append(f\"[Document {i+1}] {article_ref}\\n{doc['text']}\")\n",
        "\n",
        "        return \"\\n\\n\" + \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    def _format_article_reference(self, metadata: Dict) -> str:\n",
        "        \"\"\"Format article reference with code name and article number\"\"\"\n",
        "        parts = []\n",
        "\n",
        "        # Add code name with formatting\n",
        "        if 'code_display' in metadata:\n",
        "            parts.append(metadata['code_display'])\n",
        "        elif 'code' in metadata:\n",
        "            parts.append(metadata['code'].replace('_', ' ').title())\n",
        "\n",
        "        # Add article number\n",
        "        if 'article_number' in metadata:\n",
        "            parts.append(f\"Article {metadata['article_number']}\")\n",
        "        elif 'article_id' in metadata:\n",
        "            parts.append(f\"Article {metadata['article_id']}\")\n",
        "        elif 'reference' in metadata:\n",
        "            parts.append(metadata['reference'])\n",
        "\n",
        "        return \" - \".join(parts) if parts else \"Unknown Reference\"\n",
        "\n",
        "    def format_legal_citation(self, metadata: Dict) -> str:\n",
        "        \"\"\"Format metadata into a proper legal citation string\"\"\"\n",
        "        parts = []\n",
        "\n",
        "        # Add code/law name if available with nice formatting\n",
        "        if 'code_display' in metadata:\n",
        "            parts.append(metadata['code_display'])\n",
        "        elif 'code' in metadata:\n",
        "            parts.append(metadata['code'].replace('_', ' ').title())\n",
        "\n",
        "        # Add article reference with proper formatting\n",
        "        if 'article_number' in metadata:\n",
        "            parts.append(f\"Article {metadata['article_number']}\")\n",
        "        elif 'article_id' in metadata:\n",
        "            parts.append(f\"Article {metadata['article_id']}\")\n",
        "        elif 'reference' in metadata:\n",
        "            parts.append(f\"{metadata['reference']}\")\n",
        "\n",
        "        # Add section/chapter if available\n",
        "        section_info = []\n",
        "        if 'section' in metadata:\n",
        "            section_info.append(f\"Section {metadata['section']}\")\n",
        "        if 'chapter' in metadata:\n",
        "            section_info.append(f\"Chapter {metadata['chapter']}\")\n",
        "\n",
        "        if section_info:\n",
        "            parts.append(\", \".join(section_info))\n",
        "\n",
        "        # Add authority/jurisdiction\n",
        "        if 'authority' in metadata:\n",
        "            parts.append(f\"{metadata['authority']}\")\n",
        "\n",
        "        # Add date/version if available\n",
        "        if 'date' in metadata:\n",
        "            parts.append(f\"({metadata['date']})\")\n",
        "        elif 'version' in metadata:\n",
        "            parts.append(f\"({metadata['version']})\")\n",
        "\n",
        "        # If we still don't have any parts, use ID as fallback\n",
        "        if not parts and 'id' in metadata:\n",
        "            parts.append(f\"Document ID: {metadata['id']}\")\n",
        "\n",
        "        return \" — \".join(parts)\n",
        "\n",
        "    def answer_question(self, query: str, stream: bool = True) -> Tuple[str, List[Dict], float]:\n",
        "        \"\"\"\n",
        "        Answer a legal question using RAG pipeline\n",
        "\n",
        "        Args:\n",
        "            query: The legal question to answer\n",
        "            stream: Whether to stream the response (print tokens as they're generated)\n",
        "\n",
        "        Returns:\n",
        "            response: The LLM's response\n",
        "            documents: The retrieved documents\n",
        "            timing: Dict of timing information\n",
        "        \"\"\"\n",
        "        timing = {}\n",
        "\n",
        "        # Step 1: Retrieve relevant documents\n",
        "        start_retrieve = time.time()\n",
        "        documents = self.retrieve_documents(query)\n",
        "        timing['retrieve'] = time.time() - start_retrieve\n",
        "\n",
        "        # Step 2: Format context for the LLM\n",
        "        start_format = time.time()\n",
        "        context = self.format_context(documents)\n",
        "        timing['format'] = time.time() - start_format\n",
        "\n",
        "        # Step 3: Create prompt for LLM\n",
        "        system_prompt = self._create_legal_system_prompt()\n",
        "        prompt = f\"# Question: {query}\\n\\n# Relevant legal context:{context}\\n\\nPlease answer based only on this information.\"\n",
        "\n",
        "        # Create chat messages\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        # Apply chat template\n",
        "        text = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "        # Step 4: Generate answer with the LLM\n",
        "        sampling_params = SamplingParams(\n",
        "            temperature=0.4,\n",
        "            top_p=0.80,\n",
        "            repetition_penalty=1.2,\n",
        "            top_k=50,\n",
        "            max_tokens=512,\n",
        "        )\n",
        "\n",
        "        # Start timing\n",
        "        start_generate = time.time()\n",
        "\n",
        "        # Generate with or without streaming\n",
        "        if stream:\n",
        "            print(\"\\n\\033[1mGenerating response...\\033[0m\")\n",
        "            output = \"\"\n",
        "            for output_obj in self.llm.generate(text, sampling_params=sampling_params):\n",
        "                new_text = output_obj.outputs[0].text\n",
        "                new_token = new_text[len(output):]\n",
        "                output = new_text\n",
        "                print(new_token, end=\"\", flush=True)\n",
        "            print(\"\\n\")\n",
        "            response = output\n",
        "        else:\n",
        "            outputs = self.llm.generate(text, sampling_params=sampling_params)\n",
        "            response = outputs[0].outputs[0].text\n",
        "\n",
        "        timing['generate'] = time.time() - start_generate\n",
        "        timing['total'] = timing['retrieve'] + timing['format'] + timing['generate']\n",
        "\n",
        "        return response, documents, timing\n",
        "\n",
        "    def display_results(self, query: str, response: str, documents: List[Dict], timing: Dict,\n",
        "                        show_timing: bool = True, show_full_docs: bool = True):\n",
        "        \"\"\"\n",
        "        Display the results in a well-formatted way\n",
        "\n",
        "        Args:\n",
        "            query: The original question\n",
        "            response: The generated response\n",
        "            documents: The retrieved documents\n",
        "            timing: Dictionary with timing information\n",
        "            show_timing: Whether to display timing information\n",
        "            show_full_docs: Whether to display full document text or just snippets\n",
        "        \"\"\"\n",
        "        # Print the query\n",
        "        print(f\"\\n{'='*100}\")\n",
        "        print(f\"\\033[1mQUESTION:\\033[0m {query}\")\n",
        "        print(f\"{'='*100}\\n\")\n",
        "\n",
        "        # Print the answer\n",
        "        print(f\"\\033[1mANSWER:\\033[0m\\n{response}\")\n",
        "        print(f\"\\n{'-'*100}\\n\")\n",
        "\n",
        "        # Print the sources\n",
        "        print(f\"\\033[1mSOURCES:\\033[0m\\n\")\n",
        "        for i, doc in enumerate(documents):\n",
        "            # Format citation\n",
        "            citation = self.format_legal_citation(doc['metadata'])\n",
        "\n",
        "            print(f\"\\033[1m[{i+1}] {citation}\\033[0m\")\n",
        "\n",
        "            if show_full_docs:\n",
        "                print(f\"\\nRelevance Score: {doc['score']:.4f}\\n\")\n",
        "                print(doc['text'])\n",
        "                print(f\"\\n{'-'*50}\\n\")\n",
        "            else:\n",
        "                # Show snippet\n",
        "                snippet = doc['text'][:300] + \"...\" if len(doc['text']) > 300 else doc['text']\n",
        "                print(f\"{snippet}\\n\")\n",
        "\n",
        "        # Print timing information if requested\n",
        "        if show_timing:\n",
        "            print(f\"\\n{'='*100}\")\n",
        "            print(f\"\\033[1mPERFORMANCE METRICS:\\033[0m\")\n",
        "            print(f\"Retrieval: {timing['retrieve']:.3f}s\")\n",
        "            print(f\"Context formatting: {timing['format']:.3f}s\")\n",
        "            print(f\"Response generation: {timing['generate']:.3f}s\")\n",
        "            print(f\"Total time: {timing['total']:.3f}s\")\n",
        "\n",
        "        print(f\"\\n{'='*100}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Interactive demo of the Legal RAG Pipeline\"\"\"\n",
        "    print(\"\\n\\033[1m====== Legal RAG Assistant ======\\033[0m\\n\")\n",
        "\n",
        "    # Initialize the pipeline\n",
        "    pipeline = LegalRAGPipeline()\n",
        "\n",
        "    print(\"\\nLegal RAG Assistant is ready! Ask a legal question (or type 'quit' to exit).\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\033[1mQuestion:\\033[0m \")\n",
        "\n",
        "        if query.lower() in ('quit', 'exit', 'q'):\n",
        "            print(\"\\nThank you for using Legal RAG Assistant. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Answer the question\n",
        "        response, documents, timing = pipeline.answer_question(query, stream=True)\n",
        "\n",
        "        # Display the results\n",
        "        pipeline.display_results(query, response, documents, timing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709,
          "referenced_widgets": [
            "55af776fd3fe407fa291078944c80e40",
            "6f3dbb4914ef44339ba622e475673092",
            "0ef6fa70f13f442fae309caa94bb96a7",
            "e03fd5f95306490f9197e1b9de8367fe",
            "780447f5486d457abcc144066678d0b4",
            "1605439a21b242449026ad2b1dbd18c9",
            "cc1868a74e524d5e9cf37b92df7488a5",
            "cea2b6ba9c5644769a3506c9ac96b7da",
            "b1d5d01b3ed740ad815bebfd881e96e6",
            "5abae8c1e1194759914db6836279a2bc",
            "a6dfbe963a9948ab8690d821035bd4f4",
            "c5690086fafe42b8ba06095d3a6fd51b",
            "b7302e45a8b34463a019632a3ef32d2d",
            "e1c13cc419a64ed2a9768e5193da9754",
            "e32fb654b31c42598f42c5d1c1c99792",
            "78c1e90f3f9b4f818619421362cdfb3f",
            "d26e641199c14e0ea8ee25daee75c2f2",
            "0ed664d3691844f69f252db20d1250d3",
            "b42225a33c6f4555a24bb57250058b66",
            "78cb175615b44dbb81d8331f2b6d4f0d",
            "4a1aff604da94f6784b7055c45583e2d",
            "fde5e752b7294f11a011f9cca668fa8c"
          ]
        },
        "id": "TPN7HFqF7IFi",
        "outputId": "77eb732b-5af4-4e42-94f4-1286fafca309"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Legal RAG Pipeline...\n",
            "Loading corpus lookup...\n",
            "Loaded corpus with 1275 documents\n",
            "Loading retrieval models...\n",
            "Loading BM25+ retriever...\n",
            "BM25 Plus model loaded from /mnt/d/a_PROJECTS/legal-rag-assistant/knowledge_base/vector_store/sparse/bm25_plus.pkl\n",
            "Loading dense retriever...\n",
            "Using device: cuda for embeddings\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2ae800703894da0aea6a1de622436df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:  44%|####4     | 1.79G/4.03G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c2c6e4f70a74e40bb6d60da99c2bc4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ef938fd510240978a5cdd75a820eca7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23ab6ef755e441acbd8398ebb2a15a76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93b5f80c576c4346bb93ac974c838674",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d8d05694fe043bf9a5b805ff945cfc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dense vector index loaded from /mnt/d/a_PROJECTS/legal-rag-assistant/knowledge_base/vector_store/dense/legal_dense_index\n",
            "Loading Qwen model with VLLM...\n",
            "Using GPU: NVIDIA GeForce GTX 1650\n",
            "INFO 05-13 00:06:19 [config.py:717] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward', 'score'}. Defaulting to 'generate'.\n",
            "WARNING 05-13 00:06:19 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
            "WARNING 05-13 00:06:19 [config.py:1443] Possibly too large swap space. 4.00 GiB out of the 9.72 GiB total CPU memory is allocated for the swap space.\n",
            "INFO 05-13 00:06:19 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/mnt/d/a_PROJECTS/legal-rag-assistant/FineTuned-Qwen2/qwen-legal-assistant/merged_16bit', speculative_config=None, tokenizer='/mnt/d/a_PROJECTS/legal-rag-assistant/FineTuned-Qwen2/qwen-legal-assistant/merged_16bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/mnt/d/a_PROJECTS/legal-rag-assistant/FineTuned-Qwen2/qwen-legal-assistant/merged_16bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "WARNING 05-13 00:06:22 [interface.py:314] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
            "INFO 05-13 00:06:22 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 05-13 00:06:22 [cuda.py:289] Using XFormers backend.\n",
            "INFO 05-13 00:06:23 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 05-13 00:06:23 [model_runner.py:1108] Starting to load model /mnt/d/a_PROJECTS/legal-rag-assistant/FineTuned-Qwen2/qwen-legal-assistant/merged_16bit...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc2c0a0140654728bc9444cdb28c6d7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 05-13 00:07:43 [loader.py:458] Loading weights took 76.88 seconds\n",
            "INFO 05-13 00:07:44 [model_runner.py:1140] Model loading took 3.4654 GiB and 80.589109 seconds\n",
            "INFO 05-13 00:08:19 [worker.py:287] Memory profiling takes 34.78 seconds\n",
            "INFO 05-13 00:08:19 [worker.py:287] the current vLLM instance can use total_gpu_memory (4.00GiB) x gpu_memory_utilization (0.85) = 3.40GiB\n",
            "INFO 05-13 00:08:19 [worker.py:287] model weights take 3.47GiB; non_torch_memory takes -2.40GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 0.93GiB.\n",
            "INFO 05-13 00:08:19 [executor_base.py:112] # cuda blocks: 319, # CPU blocks: 1365\n",
            "INFO 05-13 00:08:19 [executor_base.py:117] Maximum concurrency for 4096 tokens per request: 1.25x\n",
            "INFO 05-13 00:08:25 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1e1130a59804f1c968c6d459e0961c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 05-13 00:10:01 [model_runner.py:1592] Graph capturing finished in 96 secs, took 0.00 GiB\n",
            "INFO 05-13 00:10:01 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 137.11 seconds\n",
            "Legal RAG Pipeline initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "pipeline = LegalRAGPipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "9edb1f13616e4b4ba921415d0fed15f2",
            "c8964aacbda54abb853939b43b0d664b",
            "7457675969ad4aa2bcee4cde5355eacc",
            "91a2e1ca3f2e4bd381d9d27d0ed67f64",
            "d60e190ce6e64576a78f9cedb9b828e9",
            "ecc81b312692499d8f76f889007ef1b0",
            "934d95cb71d647c890f623aabb692f3c",
            "56cec27cf7ba44bfb9e0419b342a2e1d",
            "f6846ac4065c44dba30d6a149ee37bed",
            "0b51180bcc7d466ca8400b283a8b528b",
            "0d7543ab141647a4bf538c88212e9666"
          ]
        },
        "id": "TaymzAtt76dM",
        "outputId": "e7fe7c90-c60b-4a67-f651-53ac57e62f5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1mGenerating response...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bda03a8e7b64d89ba028d693d870f1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n",
            "# Answer: Si vous êtes un magistrat : Vous pouvez être puni d’une amende de 250 à 500 EUR (pour toute personne) ; Vous devez payer cette amende; Vous recevez une amende de 750 à 1000 EUR (par exemple). Pour plus d’informations, voyez notre fiche « Qu’est-ce qui se passe si je suis un magistrat ? ». Si vous êtes un fonctionnaire public : Vous pouvez être puni d’un emprisonnement de 1 à 2 ans (et d’une amende de 250 à 500 EUR); Vous devez payer cette amende; Vous recevrez une amende de 750 à 1000 EUR (par exemple). Pour plus d’information, voyez notre fiche « Qu'est-ce qui se passe s’il y a un service public? » Si vous êtes un commandante : Vous pouvez être puni d’une amende de 250 à 500 EUR (pour toutes personnes); Vous devez payer cette amende; Vous recevrez une amende de 750 à 1000 EUR (par exemple). Pour plus d’informations, voyez notre fiche « Qu’est-ce qui se passe si j’ai commis un crime contre un enfant mineur ? ». Si vous êtes un militaire : Vous pouvez être puni d’une amende de 250 à 500 EUR (pour toutes personnes); Vous devez payer cette amende; Vous recevrez une amende de 750 à 1000 EUR (par exemple). Pour plus d’information, voyez notre fiche « Qu’est-ce qui se passe si je suis un militaire ? ».\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = \"quelle est la sanction si un outrage à un magistrat est commis lors d'une audience au tribunal ?\"\n",
        "\n",
        "# Answer the question\n",
        "response, documents, timing = pipeline.answer_question(query, stream=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "752ed1b7df3642d287e31832b362ffe3",
            "290d9e0007984546aeb42108db6cd6c4",
            "de2f2adbf005462f90f0884a69afa044",
            "c4b994bd6dd644048a7ac6a63b072f58",
            "e9a202bb490d40a094f2307dc03910ae",
            "e53a165f4fc547319aecd0b9d7b785b0",
            "e2475a5367264e1bb5d970655070f5f9",
            "d56b8b9e6f54478f82a2b32af4e32888",
            "42d1c4c5c7434f968502e45a9b08eb61",
            "659dbe9eb4da44c99b13b033c7f1d183",
            "4d21fb58477f45fe988a01e16e9d336e"
          ]
        },
        "id": "duzpuYgVHJMh",
        "outputId": "3c34f48f-a8c9-4124-d75d-4f2618bd2eb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1mGenerating response...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c782390a56894c2893975e0d902b4ec7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mQuels sont les cas où une femme peut demander le divorce sans l\u001b[39m\u001b[33m'\u001b[39m\u001b[33maccord du mari ?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Answer the question\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m response, documents, timing = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43manswer_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 264\u001b[39m, in \u001b[36mLegalRAGPipeline.answer_question\u001b[39m\u001b[34m(self, query, stream)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[33m[1mGenerating response...\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[33m[0m\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    263\u001b[39m output = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m output_obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    265\u001b[39m     new_text = output_obj.outputs[\u001b[32m0\u001b[39m].text\n\u001b[32m    266\u001b[39m     new_token = new_text[\u001b[38;5;28mlen\u001b[39m(output):]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/vllm/utils.py:1196\u001b[39m, in \u001b[36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1189\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1191\u001b[39m         warnings.warn(\n\u001b[32m   1192\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1193\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1194\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1196\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/vllm/entrypoints/llm.py:473\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[39m\n\u001b[32m    463\u001b[39m     sampling_params = \u001b[38;5;28mself\u001b[39m.get_default_sampling_params()\n\u001b[32m    465\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_and_add_requests(\n\u001b[32m    466\u001b[39m     prompts=parsed_prompts,\n\u001b[32m    467\u001b[39m     params=sampling_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    470\u001b[39m     guided_options=guided_options_request,\n\u001b[32m    471\u001b[39m     priority=priority)\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/vllm/entrypoints/llm.py:1423\u001b[39m, in \u001b[36mLLM._run_engine\u001b[39m\u001b[34m(self, use_tqdm)\u001b[39m\n\u001b[32m   1421\u001b[39m total_out_toks = \u001b[32m0\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm_engine.has_unfinished_requests():\n\u001b[32m-> \u001b[39m\u001b[32m1423\u001b[39m     step_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[32m   1425\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output.finished:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/vllm/engine/llm_engine.py:1412\u001b[39m, in \u001b[36mLLMEngine.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1408\u001b[39m     execute_model_req.async_callback = \u001b[38;5;28mself\u001b[39m.async_callbacks[\n\u001b[32m   1409\u001b[39m         virtual_engine]\n\u001b[32m   1411\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1412\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1414\u001b[39m     \u001b[38;5;28mself\u001b[39m._skip_scheduling_next_step = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InputProcessingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1416\u001b[39m     \u001b[38;5;66;03m# The input for this request cannot be processed, so we must\u001b[39;00m\n\u001b[32m   1417\u001b[39m     \u001b[38;5;66;03m# abort it. If there are remaining requests in the batch that\u001b[39;00m\n\u001b[32m   1418\u001b[39m     \u001b[38;5;66;03m# have been scheduled, they will be retried on the next step.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/vllm/executor/executor_base.py:140\u001b[39m, in \u001b[36mExecutorBase.execute_model\u001b[39m\u001b[34m(self, execute_model_req)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_model\u001b[39m(\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m, execute_model_req: ExecuteModelRequest\n\u001b[32m    139\u001b[39m ) -> Optional[List[Union[SamplerOutput, PoolerOutput]]]:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexecute_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py:56\u001b[39m, in \u001b[36mUniProcExecutor.collective_rpc\u001b[39m\u001b[34m(self, method, timeout, args, kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     55\u001b[39m     kwargs = {}\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m answer = \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/vllm/utils.py:2456\u001b[39m, in \u001b[36mrun_method\u001b[39m\u001b[34m(obj, method, args, kwargs)\u001b[39m\n\u001b[32m   2454\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2455\u001b[39m     func = partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2456\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/vllm/worker/worker_base.py:420\u001b[39m, in \u001b[36mLocalOrDistributedWorkerBase.execute_model\u001b[39m\u001b[34m(self, execute_model_req)\u001b[39m\n\u001b[32m    415\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.observability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    416\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observability_config.collect_model_execute_time):\n\u001b[32m    417\u001b[39m         orig_model_execute_time = intermediate_tensors.tensors.get(\n\u001b[32m    418\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel_execute_time\u001b[39m\u001b[33m\"\u001b[39m, torch.tensor(\u001b[32m0\u001b[39m)).item()\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvirtual_engine\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m model_execute_time = time.perf_counter() - start_time\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group().is_last_rank:\n\u001b[32m    431\u001b[39m     \u001b[38;5;66;03m# output is IntermediateTensors\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/vllm/worker/model_runner.py:1820\u001b[39m, in \u001b[36mModelRunner.execute_model\u001b[39m\u001b[34m(self, model_input, kv_caches, intermediate_tensors, num_steps, **kwargs)\u001b[39m\n\u001b[32m   1817\u001b[39m     model_input.async_callback()\n\u001b[32m   1819\u001b[39m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1820\u001b[39m output: SamplerOutput = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1821\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1822\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1823\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1824\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.observability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1825\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observability_config.collect_model_forward_time\n\u001b[32m   1826\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1827\u001b[39m     model_forward_end.synchronize()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:287\u001b[39m, in \u001b[36mSampler.forward\u001b[39m\u001b[34m(self, logits, sampling_metadata)\u001b[39m\n\u001b[32m    284\u001b[39m logprobs = torch.log_softmax(logits, dim=-\u001b[32m1\u001b[39m, dtype=torch.float)\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# Sample the next tokens.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m maybe_deferred_sample_results, maybe_sampled_tokens_tensor = \u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_modify_greedy_probs_inplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.include_gpu_probs_tensor:\n\u001b[32m    297\u001b[39m     \u001b[38;5;66;03m# Since we will defer sampler result Pythonization,\u001b[39;00m\n\u001b[32m    298\u001b[39m     \u001b[38;5;66;03m# preserve GPU-side tensors in support of later\u001b[39;00m\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m# deferred pythonization of logprobs\u001b[39;00m\n\u001b[32m    300\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m maybe_sampled_tokens_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:775\u001b[39m, in \u001b[36m_sample\u001b[39m\u001b[34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sample\u001b[39m(\n\u001b[32m    756\u001b[39m     probs: torch.Tensor,\n\u001b[32m    757\u001b[39m     logprobs: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    761\u001b[39m     modify_greedy_probs: \u001b[38;5;28mbool\u001b[39m,\n\u001b[32m    762\u001b[39m ) -> SampleReturnType:\n\u001b[32m    763\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    764\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    765\u001b[39m \u001b[33;03m        probs: (num_query_tokens_in_batch, num_vocab)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    773\u001b[39m \u001b[33;03m        sampled_token_ids_tensor: A tensor of sampled token ids.\u001b[39;00m\n\u001b[32m    774\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_with_torch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m        \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m        \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_gpu_probs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodify_greedy_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:744\u001b[39m, in \u001b[36m_sample_with_torch\u001b[39m\u001b[34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[39m\n\u001b[32m    733\u001b[39m maybe_deferred_args = SampleResultArgsType(\n\u001b[32m    734\u001b[39m     sampling_metadata=sampling_metadata,\n\u001b[32m    735\u001b[39m     sample_metadata=sample_metadata,\n\u001b[32m    736\u001b[39m     multinomial_samples=multinomial_samples,\n\u001b[32m    737\u001b[39m     greedy_samples=greedy_samples,\n\u001b[32m    738\u001b[39m     sample_results_dict=sample_results_dict)\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampling_metadata.skip_sampler_cpu_output:\n\u001b[32m    741\u001b[39m     \u001b[38;5;66;03m# GPU<->CPU sync happens here.\u001b[39;00m\n\u001b[32m    742\u001b[39m     \u001b[38;5;66;03m# This also converts the sampler output to a Python object.\u001b[39;00m\n\u001b[32m    743\u001b[39m     \u001b[38;5;66;03m# Return Pythonized sampler result & sampled token ids\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_pythonized_sample_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaybe_deferred_args\u001b[49m\u001b[43m)\u001b[49m, sampled_token_ids_tensor\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    747\u001b[39m     \u001b[38;5;66;03m# Defer sampler result Pythonization; return deferred\u001b[39;00m\n\u001b[32m    748\u001b[39m     \u001b[38;5;66;03m# Pythonization args & sampled token ids\u001b[39;00m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    750\u001b[39m         maybe_deferred_args,\n\u001b[32m    751\u001b[39m         sampled_token_ids_tensor,\n\u001b[32m    752\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:616\u001b[39m, in \u001b[36mget_pythonized_sample_results\u001b[39m\u001b[34m(sample_result_args)\u001b[39m\n\u001b[32m    614\u001b[39m         sample_results = _greedy_sample(seq_groups, greedy_samples)\n\u001b[32m    615\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;129;01min\u001b[39;00m (SamplingType.RANDOM, SamplingType.RANDOM_SEED):\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m         sample_results = \u001b[43m_random_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43mmultinomial_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43msampling_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    618\u001b[39m     sample_results_dict.update(\u001b[38;5;28mzip\u001b[39m(seq_group_id, sample_results))\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    621\u001b[39m     sample_results_dict.get(i, ([], []))\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sampling_metadata.seq_groups))\n\u001b[32m    623\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm_env/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:485\u001b[39m, in \u001b[36m_random_sample\u001b[39m\u001b[34m(selected_seq_groups, random_samples)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run random sampling on a given samples.\u001b[39;00m\n\u001b[32m    473\u001b[39m \n\u001b[32m    474\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    482\u001b[39m \u001b[33;03m    seq_group has do_sample=False, tuple contains ([], [])\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# Find the maximum n value of the prompt phase requests.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m random_samples = \u001b[43mrandom_samples\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m sample_idx = \u001b[32m0\u001b[39m\n\u001b[32m    487\u001b[39m results: SampleResultType = []\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "query = \"Quels sont les cas où une femme peut demander le divorce sans l'accord du mari ?\"\n",
        "\n",
        "# Answer the question\n",
        "response, documents, timing = pipeline.answer_question(query, stream=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "1d12b8a5863748f681995a1c3a826ddb",
            "d365a9d5723b4c079a335e6529e70a9a",
            "9d7ea2e53ff1412cbb8b42ee6847761f",
            "882839d03a664805ac5bf4ad5937dee2",
            "05364f2683574eaba8cd975f4987717d",
            "bb1260b7dfef42ce80fab404a60c9f06",
            "dc9d9791f16a4ac7ba839101490b9caa",
            "de5f5a6dd15740b19d812ab744021e9e",
            "0db980fe0c974a719dc72fe3878dcf4c",
            "9daad22927c64d2f9a5bc72ef603498a",
            "79152105f8f4449fa306833665cafc7c"
          ]
        },
        "id": "qdm32R_eHJKj",
        "outputId": "11db4b75-a9e6-4a3e-dd55-a8b4c18145fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1mGenerating response...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d12b8a5863748f681995a1c3a826ddb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n",
            "# Answer: Les peines prévues pour le vol simple sont différentes selon le type de vol. Pour le vol simple, les peines prévues sont les suivantes : Peine de réclusion : 2 ans à 10 ans de prison, ou 1 an à 2 ans de prison, si la peine de réclusion est inférieure à 1 an. Peine d'exécution : 1 an à 5 ans de prison, ou 1 an à 2 ans de prison, si la peine d'exécution est inférieure à 1 an. Peine d'obligeer ou de faire des actes de force : 1 an à 5 ans de prison, ou 1 an à 2 ans de prison, si la peine d'obligeer ou de faire des actes de force est inférieure à 1 an. Peine de dégradation de la personne : 1 an à 5 ans de prison, ou 1 an à 2 ans de prison, si la peine de dégradation de la personne est inférieure à 1 an. Peine de dégradation de la personne : 1 an à 5 ans de prison, ou 1 an à 2 ans de prison, si la peine de dégradation de la personne est inférieure à 1 an. Peine de dégradation de la personne : 1 an à 5 ans de prison, ou 1 an à 2 ans de prison, si la peine de dégradation de la personne est inférieure à 1 an. Peine de dégradation de la personne : 1 an à 5 ans de prison, ou 1 an à 2 ans de prison, si la peine de dégradation de la personne est inférieure à 1 an. Peine de dégradation de la personne : 1 an à 5 ans de prison, ou 1 an à 2 ans de prison, si la peine de dégradation de la personne est inférieure à 1 an. Peine de dégradation de la personne : 1 an à 5 ans de prison, ou 1 an à 2 ans de prison, si la peine de dégradation de la personne est inférieure à 1 an. Peine de dégradation de la personne : 1 an à 5 ans de prison, ou 1 an à 2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "query = \"Quelles sont les peines prévues pour le vol simple\"\n",
        "\n",
        "# Answer the question\n",
        "response, documents, timing = pipeline.answer_question(query, stream=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "984f481a48804ae1b1538ab3753c1d66",
            "e560f51834da4ea197b636c99e6c117e",
            "9779ee352ba34f97b16228cab2559888",
            "5819fc03475746989c4a03f4cbce0875",
            "7b3d58fdbfdb467ea5e39c24aa6c4da0",
            "8085c9835916400681cc1f845b23d514",
            "534a92cb843a4ab18bab0e834ef92e1c",
            "e7546bd2e5f3431c996d3418a9b61b08",
            "9ebd7120feff4918b5dc9c91235e0077",
            "46603c37e36e469c847f3d5622dbb1c3",
            "8cae02eae8be4a35939895e2d5c2cfcf"
          ]
        },
        "id": "IPBD5AmvMHbi",
        "outputId": "d9508468-861e-4527-cf21-2bd4ca0daa19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1mGenerating response...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "984f481a48804ae1b1538ab3753c1d66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant\n",
            "# Answer: Avant d’aller plus loin Seuls les victimes de violence sexuelle et les victimes de violence psychologique sont soumises aux peines. Les victimes de violence sexuelle sont soumises aux peines de 1 à 3 ans de prison. Les victimes de violence psychologique sont soumises aux peines de 1 à 3 ans de prison. Les victimes de violence visée aux 3 documents sont soumises aux peines de 1 à 3 ans de prison. Les victimes de violence visée aux 2 documents sont soumises aux peines de 5 à 30 ans de prison. Les victimes de violence visée aux 1 document sont soumises aux peines de 100.000 dirhams à 100.000 dirhams. Les victimes de violence visée aux 2 documents sont soumises aux peines de 100.000 dirhams à 100.000 dirhams. Les victimes de violence visée aux 3 documents sont soumises aux peines de 100.000 dirhams à 100.000 dirhams. Les victimes de violence visée aux 1 document sont soumises aux peines de 100.000 dirhams à 100.000 dirhams. Les victimes de violence visée aux 2 documents sont soumises aux peines de 100.000 dirhams à 100.000 dirhams.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "query = \"Quelles sont les peines prévues pour le vol simple selon le Code pénal marocain ?\"\n",
        "\n",
        "# Answer the question\n",
        "response, documents, timing = pipeline.answer_question(query, stream=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKL8jLLOAtam",
        "outputId": "b8ad9158-4c67-4900-be1f-c4c4c7474375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================================================================\n",
            "\u001b[1mQUESTION:\u001b[0m Quelles sont les peines prévues pour le vol simple\n",
            "====================================================================================================\n",
            "\n",
            "\u001b[1mANSWER:\u001b[0m\n",
            "assistant\n",
            "# Answer: Les peines prévues pour le vol simple sont différentes selon le type de vol. Pour le vol simple, les peines prévues sont les suivantes : Peine de réclusion : 2 ans à 10 ans de prison, ou 1 an à 2 ans de prison, si la peine de réclusion est inférieure à 1 an. Peine d'exécution : 1 an à 5 ans de prison, ou 1 an à 2 ans de prison, si la peine d'exécution est inférieure à 1 an. Peine d'obligeer ou de faire des actes de force : 1 an à 5 ans de prison, ou 1 an à 2 ans de prison, si la peine d'obligeer ou de faire des actes de force est inférieure à 1 an. Peine de dégradation de la personne : 1 an à 5 ans de prison, ou 1 an à 2 ans de prison, si la peine de dégradation de la personne est inférieure à 1 an. Peine de dégradation de la personne : 1 an à 5 ans de prison, ou 1 an à 2 ans de prison, si la peine de dégradation de la personne est inférieure à 1 an. Peine de dégradation de la personne : 1 an à 5 ans de prison, ou 1 an à 2 ans de prison, si la peine de dégradation de la personne est inférieure à 1 an. Peine de dégradation de la personne : 1 an à 5 ans de prison, ou 1 an à 2 ans de prison, si la peine de dégradation de la personne est inférieure à 1 an. Peine de dégradation de la personne : 1 an à 5 ans de prison, ou 1 an à 2 ans de prison, si la peine de dégradation de la personne est inférieure à 1 an. Peine de dégradation de la personne : 1 an à 5 ans de prison, ou 1 an à 2 ans de prison, si la peine de dégradation de la personne est inférieure à 1 an. Peine de dégradation de la personne : 1 an à 5 ans de prison, ou 1 an à 2\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mSOURCES:\u001b[0m\n",
            "\n",
            "\u001b[1m[1] Code Penale 2018 — Article 306\u001b[0m\n",
            "\n",
            "Relevance Score: 0.0280\n",
            "\n",
            "Les personnes morales ne peuvent être condamnées qu'à des peines \n",
            "pécuniaires et aux peines accessoires prévues sous les numéros 5, 6 et 7 \n",
            "de l'article 36. Elles peuvent également être soumises aux mesures de \n",
            "sûreté réelles de l'article 62.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\u001b[1m[2] Code Penale 2018 — Article 196\u001b[0m\n",
            "\n",
            "Relevance Score: 0.0274\n",
            "\n",
            "Les peines criminelles principales sont :\n",
            "1° La mort;\n",
            "2° La réclusion perpétuelle;\n",
            "3° La réclusion à temps pour une durée de cinq à trente ans;\n",
            "4° La résidence forcée;\n",
            "5° La dégradation civique.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\u001b[1m[3] Code Penale 2018 — Article 383 — Section SECTION III DES CRIMES ET DELITS CONTRE LA SURETE INTERIEURE DE L'ETAT\u001b[0m\n",
            "\n",
            "Relevance Score: 0.0268\n",
            "\n",
            "Dans le cas où l'un des crimes prévus à l'article 201 a été exécuté ou \n",
            "simplement tenté par une bande, les peines édictées à cet article sont, \n",
            "dans les conditions prévues à l'article 171, appliquées à tous individus \n",
            "sans distinction de grades faisant partie de la bande.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "\u001b[1mPERFORMANCE METRICS:\u001b[0m\n",
            "Retrieval: 0.505s\n",
            "Context formatting: 0.000s\n",
            "Response generation: 8.225s\n",
            "Total time: 8.730s\n",
            "\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Display the results\n",
        "pipeline.display_results(query, response, documents, timing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcJxyyBXAxty"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11 (vLLM)",
      "language": "python",
      "name": "vllm_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05364f2683574eaba8cd975f4987717d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "0b51180bcc7d466ca8400b283a8b528b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d7543ab141647a4bf538c88212e9666": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0db980fe0c974a719dc72fe3878dcf4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ed664d3691844f69f252db20d1250d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ef6fa70f13f442fae309caa94bb96a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cea2b6ba9c5644769a3506c9ac96b7da",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1d5d01b3ed740ad815bebfd881e96e6",
            "value": 1
          }
        },
        "1605439a21b242449026ad2b1dbd18c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d12b8a5863748f681995a1c3a826ddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d365a9d5723b4c079a335e6529e70a9a",
              "IPY_MODEL_9d7ea2e53ff1412cbb8b42ee6847761f",
              "IPY_MODEL_882839d03a664805ac5bf4ad5937dee2"
            ],
            "layout": "IPY_MODEL_05364f2683574eaba8cd975f4987717d"
          }
        },
        "290d9e0007984546aeb42108db6cd6c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e53a165f4fc547319aecd0b9d7b785b0",
            "placeholder": "​",
            "style": "IPY_MODEL_e2475a5367264e1bb5d970655070f5f9",
            "value": "Processed prompts: 100%"
          }
        },
        "42d1c4c5c7434f968502e45a9b08eb61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46603c37e36e469c847f3d5622dbb1c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a1aff604da94f6784b7055c45583e2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d21fb58477f45fe988a01e16e9d336e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "534a92cb843a4ab18bab0e834ef92e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55af776fd3fe407fa291078944c80e40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f3dbb4914ef44339ba622e475673092",
              "IPY_MODEL_0ef6fa70f13f442fae309caa94bb96a7",
              "IPY_MODEL_e03fd5f95306490f9197e1b9de8367fe"
            ],
            "layout": "IPY_MODEL_780447f5486d457abcc144066678d0b4"
          }
        },
        "56cec27cf7ba44bfb9e0419b342a2e1d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5819fc03475746989c4a03f4cbce0875": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46603c37e36e469c847f3d5622dbb1c3",
            "placeholder": "​",
            "style": "IPY_MODEL_8cae02eae8be4a35939895e2d5c2cfcf",
            "value": " 1/1 [00:06&lt;00:00,  6.99s/it, est. speed input: 177.58 toks/s, output: 51.62 toks/s]"
          }
        },
        "5abae8c1e1194759914db6836279a2bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "659dbe9eb4da44c99b13b033c7f1d183": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f3dbb4914ef44339ba622e475673092": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1605439a21b242449026ad2b1dbd18c9",
            "placeholder": "​",
            "style": "IPY_MODEL_cc1868a74e524d5e9cf37b92df7488a5",
            "value": ""
          }
        },
        "7457675969ad4aa2bcee4cde5355eacc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56cec27cf7ba44bfb9e0419b342a2e1d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6846ac4065c44dba30d6a149ee37bed",
            "value": 1
          }
        },
        "752ed1b7df3642d287e31832b362ffe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_290d9e0007984546aeb42108db6cd6c4",
              "IPY_MODEL_de2f2adbf005462f90f0884a69afa044",
              "IPY_MODEL_c4b994bd6dd644048a7ac6a63b072f58"
            ],
            "layout": "IPY_MODEL_e9a202bb490d40a094f2307dc03910ae"
          }
        },
        "780447f5486d457abcc144066678d0b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78c1e90f3f9b4f818619421362cdfb3f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78cb175615b44dbb81d8331f2b6d4f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79152105f8f4449fa306833665cafc7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b3d58fdbfdb467ea5e39c24aa6c4da0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "8085c9835916400681cc1f845b23d514": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "882839d03a664805ac5bf4ad5937dee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9daad22927c64d2f9a5bc72ef603498a",
            "placeholder": "​",
            "style": "IPY_MODEL_79152105f8f4449fa306833665cafc7c",
            "value": " 1/1 [00:08&lt;00:00,  8.21s/it, est. speed input: 54.54 toks/s, output: 62.33 toks/s]"
          }
        },
        "8cae02eae8be4a35939895e2d5c2cfcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91a2e1ca3f2e4bd381d9d27d0ed67f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b51180bcc7d466ca8400b283a8b528b",
            "placeholder": "​",
            "style": "IPY_MODEL_0d7543ab141647a4bf538c88212e9666",
            "value": " 1/1 [00:08&lt;00:00,  8.32s/it, est. speed input: 113.13 toks/s, output: 61.56 toks/s]"
          }
        },
        "934d95cb71d647c890f623aabb692f3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9779ee352ba34f97b16228cab2559888": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7546bd2e5f3431c996d3418a9b61b08",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ebd7120feff4918b5dc9c91235e0077",
            "value": 1
          }
        },
        "984f481a48804ae1b1538ab3753c1d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e560f51834da4ea197b636c99e6c117e",
              "IPY_MODEL_9779ee352ba34f97b16228cab2559888",
              "IPY_MODEL_5819fc03475746989c4a03f4cbce0875"
            ],
            "layout": "IPY_MODEL_7b3d58fdbfdb467ea5e39c24aa6c4da0"
          }
        },
        "9d7ea2e53ff1412cbb8b42ee6847761f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de5f5a6dd15740b19d812ab744021e9e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0db980fe0c974a719dc72fe3878dcf4c",
            "value": 1
          }
        },
        "9daad22927c64d2f9a5bc72ef603498a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ebd7120feff4918b5dc9c91235e0077": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9edb1f13616e4b4ba921415d0fed15f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8964aacbda54abb853939b43b0d664b",
              "IPY_MODEL_7457675969ad4aa2bcee4cde5355eacc",
              "IPY_MODEL_91a2e1ca3f2e4bd381d9d27d0ed67f64"
            ],
            "layout": "IPY_MODEL_d60e190ce6e64576a78f9cedb9b828e9"
          }
        },
        "a6dfbe963a9948ab8690d821035bd4f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1d5d01b3ed740ad815bebfd881e96e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b42225a33c6f4555a24bb57250058b66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7302e45a8b34463a019632a3ef32d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d26e641199c14e0ea8ee25daee75c2f2",
            "placeholder": "​",
            "style": "IPY_MODEL_0ed664d3691844f69f252db20d1250d3",
            "value": "Capturing CUDA graph shapes: 100%"
          }
        },
        "bb1260b7dfef42ce80fab404a60c9f06": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4b994bd6dd644048a7ac6a63b072f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_659dbe9eb4da44c99b13b033c7f1d183",
            "placeholder": "​",
            "style": "IPY_MODEL_4d21fb58477f45fe988a01e16e9d336e",
            "value": " 1/1 [00:06&lt;00:00,  6.07s/it, est. speed input: 98.41 toks/s, output: 61.16 toks/s]"
          }
        },
        "c5690086fafe42b8ba06095d3a6fd51b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7302e45a8b34463a019632a3ef32d2d",
              "IPY_MODEL_e1c13cc419a64ed2a9768e5193da9754",
              "IPY_MODEL_e32fb654b31c42598f42c5d1c1c99792"
            ],
            "layout": "IPY_MODEL_78c1e90f3f9b4f818619421362cdfb3f"
          }
        },
        "c8964aacbda54abb853939b43b0d664b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecc81b312692499d8f76f889007ef1b0",
            "placeholder": "​",
            "style": "IPY_MODEL_934d95cb71d647c890f623aabb692f3c",
            "value": "Processed prompts: 100%"
          }
        },
        "cc1868a74e524d5e9cf37b92df7488a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cea2b6ba9c5644769a3506c9ac96b7da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d26e641199c14e0ea8ee25daee75c2f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d365a9d5723b4c079a335e6529e70a9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb1260b7dfef42ce80fab404a60c9f06",
            "placeholder": "​",
            "style": "IPY_MODEL_dc9d9791f16a4ac7ba839101490b9caa",
            "value": "Processed prompts: 100%"
          }
        },
        "d56b8b9e6f54478f82a2b32af4e32888": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d60e190ce6e64576a78f9cedb9b828e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "dc9d9791f16a4ac7ba839101490b9caa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de2f2adbf005462f90f0884a69afa044": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d56b8b9e6f54478f82a2b32af4e32888",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42d1c4c5c7434f968502e45a9b08eb61",
            "value": 1
          }
        },
        "de5f5a6dd15740b19d812ab744021e9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e03fd5f95306490f9197e1b9de8367fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5abae8c1e1194759914db6836279a2bc",
            "placeholder": "​",
            "style": "IPY_MODEL_a6dfbe963a9948ab8690d821035bd4f4",
            "value": "Loading pt checkpoint shards: 100% Completed | 1/1 [00:19&lt;00:00, 19.38s/it]\n"
          }
        },
        "e1c13cc419a64ed2a9768e5193da9754": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b42225a33c6f4555a24bb57250058b66",
            "max": 35,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78cb175615b44dbb81d8331f2b6d4f0d",
            "value": 35
          }
        },
        "e2475a5367264e1bb5d970655070f5f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e32fb654b31c42598f42c5d1c1c99792": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a1aff604da94f6784b7055c45583e2d",
            "placeholder": "​",
            "style": "IPY_MODEL_fde5e752b7294f11a011f9cca668fa8c",
            "value": " 35/35 [00:43&lt;00:00,  1.08s/it]"
          }
        },
        "e53a165f4fc547319aecd0b9d7b785b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e560f51834da4ea197b636c99e6c117e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8085c9835916400681cc1f845b23d514",
            "placeholder": "​",
            "style": "IPY_MODEL_534a92cb843a4ab18bab0e834ef92e1c",
            "value": "Processed prompts: 100%"
          }
        },
        "e7546bd2e5f3431c996d3418a9b61b08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9a202bb490d40a094f2307dc03910ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "ecc81b312692499d8f76f889007ef1b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6846ac4065c44dba30d6a149ee37bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fde5e752b7294f11a011f9cca668fa8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
